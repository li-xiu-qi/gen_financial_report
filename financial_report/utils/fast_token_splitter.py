"""
é«˜æ€§èƒ½Tokenåˆ†å—å™¨
ä¼˜åŒ–äº†tokenè®¡ç®—å’Œå­—ç¬¦ä¸²æ“ä½œçš„æ€§èƒ½
"""
import re
from typing import List, Optional, Tuple
from .calculate_tokens import TokenCalculator, OpenAITokenCalculator


class FastTokenSplitter:
    """
    é«˜æ€§èƒ½çš„Tokenåˆ†å—å™¨ï¼Œä¼˜åŒ–äº†ä»¥ä¸‹æ–¹é¢ï¼š
    1. å‡å°‘tokenè®¡ç®—æ¬¡æ•°
    2. ä¼˜åŒ–å­—ç¬¦ä¸²æ“ä½œ
    3. ä½¿ç”¨äºŒåˆ†æŸ¥æ‰¾ç¡®å®šåˆ†å—è¾¹ç•Œ
    4. ç¼“å­˜tokenè®¡ç®—ç»“æœ
    """
    
    def __init__(
        self,
        token_calculator: TokenCalculator,
        chunk_size: int = 500,
        chunk_overlap: int = 50,
        separators: Optional[List[str]] = None
    ):
        self.token_calculator = token_calculator
        self.chunk_size = chunk_size
        self.chunk_overlap = chunk_overlap
        
        # ä¼˜åŒ–çš„åˆ†éš”ç¬¦åˆ—è¡¨ï¼ŒæŒ‰ä¼˜å…ˆçº§æ’åº
        self.separators = separators or [
            "\n\n",  # æ®µè½åˆ†éš”
            "ã€‚", "ï¼", "ï¼Ÿ", "ï¼›",  # ä¸­æ–‡å¥å­ç»“æŸ
            ". ", "! ", "? ", "; ",  # è‹±æ–‡å¥å­ç»“æŸ
            "\n",  # è¡Œåˆ†éš”
            "ï¼Œ", "ã€", ", ",  # çŸ­è¯­åˆ†éš”
            " ",  # è¯åˆ†éš”
        ]
        
        # Tokenè®¡ç®—ç¼“å­˜
        self._token_cache = {}
        self._cache_hits = 0
        self._cache_misses = 0
    
    def _get_token_count_cached(self, text: str) -> int:
        """å¸¦ç¼“å­˜çš„tokenè®¡ç®—"""
        if text in self._token_cache:
            self._cache_hits += 1
            return self._token_cache[text]
        
        self._cache_misses += 1
        count = self.token_calculator.count_tokens(text)
        
        # é™åˆ¶ç¼“å­˜å¤§å°ï¼Œé¿å…å†…å­˜æ³„æ¼
        if len(self._token_cache) > 10000:
            # æ¸…ç†ä¸€åŠçš„ç¼“å­˜
            keys_to_remove = list(self._token_cache.keys())[:5000]
            for key in keys_to_remove:
                del self._token_cache[key]
        
        self._token_cache[text] = count
        return count
    
    def _find_best_split_point(self, text: str, max_chars: int) -> int:
        """
        ä½¿ç”¨äºŒåˆ†æŸ¥æ‰¾æ‰¾åˆ°æœ€ä½³åˆ†å‰²ç‚¹
        è¿”å›ä¸è¶…è¿‡tokené™åˆ¶çš„æœ€å¤§å­—ç¬¦ä½ç½®
        """
        if len(text) <= max_chars:
            if self._get_token_count_cached(text) <= self.chunk_size:
                return len(text)
        
        # äºŒåˆ†æŸ¥æ‰¾æœ€ä½³åˆ†å‰²ç‚¹
        left, right = 0, min(len(text), max_chars)
        best_pos = 0
        
        while left <= right:
            mid = (left + right) // 2
            chunk = text[:mid]
            
            if self._get_token_count_cached(chunk) <= self.chunk_size:
                best_pos = mid
                left = mid + 1
            else:
                right = mid - 1
        
        return best_pos
    
    def _find_separator_near_position(self, text: str, target_pos: int, search_range: int = 200) -> int:
        """
        åœ¨ç›®æ ‡ä½ç½®é™„è¿‘å¯»æ‰¾æœ€ä½³çš„åˆ†éš”ç¬¦ä½ç½®
        """
        # æœç´¢èŒƒå›´ï¼štarget_poså‰åsearch_rangeä¸ªå­—ç¬¦
        start_search = max(0, target_pos - search_range)
        end_search = min(len(text), target_pos + search_range)
        search_text = text[start_search:end_search]
        
        best_pos = target_pos
        best_priority = len(self.separators)  # æœ€ä½ä¼˜å…ˆçº§
        
        for priority, separator in enumerate(self.separators):
            # ä»ç›®æ ‡ä½ç½®å‘å‰æœç´¢åˆ†éš”ç¬¦
            for match in re.finditer(re.escape(separator), search_text):
                abs_pos = start_search + match.end()  # åˆ†éš”ç¬¦åçš„ä½ç½®
                
                # ä¼˜å…ˆé€‰æ‹©æ›´æ¥è¿‘ç›®æ ‡ä½ç½®ä¸”ä¼˜å…ˆçº§æ›´é«˜çš„åˆ†éš”ç¬¦
                if priority < best_priority or (priority == best_priority and abs(abs_pos - target_pos) < abs(best_pos - target_pos)):
                    # ç¡®ä¿åˆ†å‰²åçš„chunkä¸è¶…è¿‡tokené™åˆ¶
                    if self._get_token_count_cached(text[:abs_pos]) <= self.chunk_size:
                        best_pos = abs_pos
                        best_priority = priority
        
        return best_pos
    
    def split_text(self, text: str) -> List[str]:
        """
        é«˜æ€§èƒ½æ–‡æœ¬åˆ†å‰²
        """
        if not text:
            return []
        
        # å¦‚æœæ•´ä¸ªæ–‡æœ¬éƒ½åœ¨é™åˆ¶å†…ï¼Œç›´æ¥è¿”å›
        if self._get_token_count_cached(text) <= self.chunk_size:
            return [text]
        
        chunks = []
        current_pos = 0
        text_len = len(text)
        
        # ä¼°ç®—æ¯ä¸ªtokençš„å¹³å‡å­—ç¬¦æ•°ï¼ˆç”¨äºåˆå§‹ä¼°ç®—ï¼‰
        sample_text = text[:min(1000, len(text))]
        sample_tokens = self._get_token_count_cached(sample_text)
        avg_chars_per_token = len(sample_text) / max(sample_tokens, 1)
        estimated_chars_per_chunk = int(self.chunk_size * avg_chars_per_token * 1.2)  # ç•™ä¸€äº›ä½™é‡
        
        while current_pos < text_len:
            # ä¼°ç®—è¿™ä¸ªchunkçš„ç»“æŸä½ç½®
            estimated_end = min(current_pos + estimated_chars_per_chunk, text_len)
            
            # ä½¿ç”¨äºŒåˆ†æŸ¥æ‰¾æ‰¾åˆ°ç²¾ç¡®çš„åˆ†å‰²ç‚¹
            relative_end = self._find_best_split_point(
                text[current_pos:], 
                estimated_end - current_pos
            )
            absolute_end = current_pos + relative_end
            
            # å¦‚æœä¸æ˜¯æœ€åä¸€ä¸ªchunkï¼Œå°è¯•åœ¨åˆ†éš”ç¬¦å¤„åˆ†å‰²
            if absolute_end < text_len:
                better_end = self._find_separator_near_position(
                    text, absolute_end, search_range=min(200, estimated_chars_per_chunk // 4)
                )
                if better_end > current_pos:  # ç¡®ä¿æœ‰è¿›å±•
                    absolute_end = better_end
            
            # æå–chunk
            chunk = text[current_pos:absolute_end].strip()
            if chunk:
                chunks.append(chunk)
            
            # ç§»åŠ¨åˆ°ä¸‹ä¸€ä¸ªä½ç½®
            if absolute_end == current_pos:  # é¿å…æ­»å¾ªç¯
                current_pos += 1
            else:
                current_pos = absolute_end
        
        # æ·»åŠ é‡å ï¼ˆå¦‚æœéœ€è¦ï¼‰
        if self.chunk_overlap > 0 and len(chunks) > 1:
            chunks = self._add_overlap_optimized(chunks)
        
        return chunks
    
    def _add_overlap_optimized(self, chunks: List[str]) -> List[str]:
        """
        ä¼˜åŒ–çš„é‡å æ·»åŠ ï¼Œå‡å°‘å­—ç¬¦ä¸²æ“ä½œ
        """
        if not chunks or len(chunks) <= 1:
            return chunks
        
        overlapped_chunks = [chunks[0]]
        
        for i in range(1, len(chunks)):
            prev_chunk = chunks[i-1]
            current_chunk = chunks[i]
            
            # è®¡ç®—é‡å éƒ¨åˆ†
            if len(prev_chunk) > self.chunk_overlap:
                overlap = prev_chunk[-self.chunk_overlap:]
            else:
                overlap = prev_chunk
            
            # ç»„åˆchunk
            combined = overlap + current_chunk
            
            # å¦‚æœç»„åˆåè¶…è¿‡tokené™åˆ¶ï¼Œå‡å°‘é‡å 
            if self._get_token_count_cached(combined) > self.chunk_size:
                # äºŒåˆ†æŸ¥æ‰¾åˆé€‚çš„é‡å é•¿åº¦
                left, right = 0, len(overlap)
                best_overlap_len = 0
                
                while left <= right:
                    mid = (left + right) // 2
                    test_overlap = overlap[-mid:] if mid > 0 else ""
                    test_combined = test_overlap + current_chunk
                    
                    if self._get_token_count_cached(test_combined) <= self.chunk_size:
                        best_overlap_len = mid
                        left = mid + 1
                    else:
                        right = mid - 1
                
                if best_overlap_len > 0:
                    final_overlap = overlap[-best_overlap_len:]
                    overlapped_chunks.append(final_overlap + current_chunk)
                else:
                    overlapped_chunks.append(current_chunk)
            else:
                overlapped_chunks.append(combined)
        
        return overlapped_chunks
    
    def get_cache_stats(self) -> dict:
        """è·å–ç¼“å­˜ç»Ÿè®¡ä¿¡æ¯"""
        total_requests = self._cache_hits + self._cache_misses
        hit_rate = self._cache_hits / total_requests if total_requests > 0 else 0
        
        return {
            "cache_hits": self._cache_hits,
            "cache_misses": self._cache_misses,
            "hit_rate": f"{hit_rate:.2%}",
            "cache_size": len(self._token_cache)
        }
    
    def clear_cache(self):
        """æ¸…ç©ºç¼“å­˜"""
        self._token_cache.clear()
        self._cache_hits = 0
        self._cache_misses = 0


# ä¾¿æ·å‡½æ•°
def fast_split_text(
    text: str,
    token_calculator: TokenCalculator,
    chunk_size: int = 500,
    chunk_overlap: int = 50
) -> List[str]:
    """
    ä¾¿æ·çš„æ–‡æœ¬åˆ†å‰²å‡½æ•°
    """
    splitter = FastTokenSplitter(
        token_calculator=token_calculator,
        chunk_size=chunk_size,
        chunk_overlap=chunk_overlap
    )
    return splitter.split_text(text)


# æ€§èƒ½æµ‹è¯•å‡½æ•°
def benchmark_splitters(text: str, token_calculator: TokenCalculator, chunk_size: int = 500):
    """
    å¯¹æ¯”æ–°æ—§åˆ†å‰²å™¨çš„æ€§èƒ½
    """
    import time
    
    print("ğŸ”¬ æ€§èƒ½æµ‹è¯•å¼€å§‹...")
    print(f"ğŸ“„ æ–‡æœ¬é•¿åº¦: {len(text)} å­—ç¬¦")
    print(f"ğŸ¯ ç›®æ ‡chunkå¤§å°: {chunk_size} tokens")
    
    # æµ‹è¯•æ–°çš„å¿«é€Ÿåˆ†å‰²å™¨
    print("\nğŸš€ æµ‹è¯• FastTokenSplitter...")
    fast_splitter = FastTokenSplitter(token_calculator, chunk_size=chunk_size)
    
    start_time = time.time()
    fast_chunks = fast_splitter.split_text(text)
    fast_time = time.time() - start_time
    
    fast_stats = fast_splitter.get_cache_stats()
    
    print(f"   â±ï¸  è€—æ—¶: {fast_time:.3f}ç§’")
    print(f"   ğŸ“Š ç”Ÿæˆchunks: {len(fast_chunks)}")
    print(f"   ğŸ¯ ç¼“å­˜å‘½ä¸­ç‡: {fast_stats['hit_rate']}")
    print(f"   ğŸ’¾ ç¼“å­˜å¤§å°: {fast_stats['cache_size']}")
    
    # æµ‹è¯•åŸå§‹çš„é€’å½’åˆ†å‰²å™¨
    print("\nğŸŒ æµ‹è¯• RecursiveTokenTextSplitter...")
    from .recursive_token_splitter import RecursiveTokenTextSplitter
    recursive_splitter = RecursiveTokenTextSplitter(token_calculator, chunk_size=chunk_size)
    
    start_time = time.time()
    recursive_chunks = recursive_splitter.split_text(text)
    recursive_time = time.time() - start_time
    
    print(f"   â±ï¸  è€—æ—¶: {recursive_time:.3f}ç§’")
    print(f"   ğŸ“Š ç”Ÿæˆchunks: {len(recursive_chunks)}")
    
    # æ€§èƒ½å¯¹æ¯”
    speedup = recursive_time / fast_time if fast_time > 0 else float('inf')
    print(f"\nğŸ“ˆ æ€§èƒ½æå‡: {speedup:.1f}x å€")
    
    return {
        "fast_time": fast_time,
        "recursive_time": recursive_time,
        "speedup": speedup,
        "fast_chunks": len(fast_chunks),
        "recursive_chunks": len(recursive_chunks)
    }


if __name__ == "__main__":
    # ç®€å•æµ‹è¯•
    from .calculate_tokens import OpenAITokenCalculator
    
    calculator = OpenAITokenCalculator()
    test_text = "è¿™æ˜¯ä¸€ä¸ªæµ‹è¯•æ–‡æœ¬ã€‚" * 1000  # åˆ›å»ºä¸€ä¸ªè¾ƒé•¿çš„æµ‹è¯•æ–‡æœ¬
    
    # è¿è¡Œæ€§èƒ½æµ‹è¯•
    benchmark_splitters(test_text, calculator, chunk_size=200)