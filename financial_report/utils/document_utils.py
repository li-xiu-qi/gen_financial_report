# document_utils.py
import asyncio
from typing import List, Dict, Any, Optional, Set, Tuple
from dataclasses import asdict
from .document_types import PreDoc, Doc

# 2. ç”Ÿæˆæ‘˜è¦ï¼ˆä½¿ç”¨ info_description.generate_full_content_descriptionï¼Œåˆ†å—å¤„ç†ï¼‰
from ..llm_calls.info_description import generate_full_content_description, async_generate_full_content_description
from .calculate_tokens import TokenCalculator, OpenAITokenCalculator


async def _generate_summary_async(
    content: str,
    api_key: str,
    base_url: str,
    model: str,
    semaphore: asyncio.Semaphore,
    progress_counter: dict = None,
    task_id: str = ""
) -> str:
    """å¼‚æ­¥ç”Ÿæˆæ‘˜è¦ï¼Œå¸¦å¹¶å‘æ§åˆ¶å’Œè¿›åº¦æ˜¾ç¤º"""
    async with semaphore:
        if progress_counter:
            print(f"   ğŸ”„ [{progress_counter['current']}/{progress_counter['total']}] æ­£åœ¨ç”Ÿæˆæ‘˜è¦: {task_id[:50]}...")
        
        # ä½¿ç”¨çœŸæ­£çš„å¼‚æ­¥å‡½æ•°
        try:
            result = await async_generate_full_content_description(
                content=content,
                api_key=api_key,
                base_url=base_url,
                model=model
            )
            
            if progress_counter:
                progress_counter['current'] += 1
                progress_counter['completed'] += 1
                print(f"   âœ… [{progress_counter['current']}/{progress_counter['total']}] æ‘˜è¦ç”Ÿæˆå®Œæˆ: {task_id[:50]}...")
            
            return result
        except Exception as e:
            if progress_counter:
                progress_counter['current'] += 1
                progress_counter['failed'] += 1
                print(f"   âŒ [{progress_counter['current']}/{progress_counter['total']}] æ‘˜è¦ç”Ÿæˆå¤±è´¥: {task_id[:50]}... (é”™è¯¯: {str(e)[:100]})")
            raise e


async def _calculate_tokens_async(
    docs: List[PreDoc],
    token_calculator: TokenCalculator,
    max_concurrent: int = 100
):
    """å¼‚æ­¥å¹¶å‘è®¡ç®—æ‰€æœ‰æ–‡æ¡£çš„tokenæ•°é‡"""
    print("ğŸ”¢ å¹¶å‘è®¡ç®—æ–‡æ¡£tokenæ•°é‡...")
    
    semaphore = asyncio.Semaphore(max_concurrent)
    
    async def _calculate_single_doc_tokens(doc: PreDoc, index: int, total: int):
        async with semaphore:
            # åœ¨çº¿ç¨‹æ± ä¸­æ‰§è¡Œtokenè®¡ç®—ï¼ˆå› ä¸ºtokenizeré€šå¸¸æ˜¯CPUå¯†é›†å‹ï¼‰
            loop = asyncio.get_event_loop()
            
            # åªè®¡ç®—è¿˜æ²¡æœ‰tokenæ•°é‡çš„å­—æ®µ
            tasks = []
            
            if doc.content and doc.content_tokens is None:
                tasks.append(('content', loop.run_in_executor(None, token_calculator.count_tokens, doc.content)))
            
            if doc.summary and doc.summary_tokens is None:
                tasks.append(('summary', loop.run_in_executor(None, token_calculator.count_tokens, doc.summary)))
            
            if doc.raw_content and doc.raw_content_tokens is None:
                tasks.append(('raw_content', loop.run_in_executor(None, token_calculator.count_tokens, doc.raw_content)))
            
            # å¹¶å‘æ‰§è¡Œæ‰€æœ‰tokenè®¡ç®—ä»»åŠ¡
            if tasks:
                results = await asyncio.gather(*[task[1] for task in tasks])
                for (field_name, _), token_count in zip(tasks, results):
                    if field_name == 'content':
                        doc.content_tokens = token_count
                    elif field_name == 'summary':
                        doc.summary_tokens = token_count
                    elif field_name == 'raw_content':
                        doc.raw_content_tokens = token_count
            
            # æ˜¾ç¤ºè¿›åº¦
            if (index + 1) % 20 == 0 or (index + 1) == total:
                print(f"   ğŸ”¢ [{index + 1}/{total}] Tokenè®¡ç®—è¿›åº¦: {((index + 1)/total*100):.1f}%")
    
    # åˆ›å»ºæ‰€æœ‰tokenè®¡ç®—ä»»åŠ¡
    tasks = [
        _calculate_single_doc_tokens(doc, i, len(docs))
        for i, doc in enumerate(docs)
    ]
    
    # å¹¶å‘æ‰§è¡Œæ‰€æœ‰ä»»åŠ¡
    await asyncio.gather(*tasks)
    
    # ç»Ÿè®¡ç»“æœ
    calculated_count = sum(1 for doc in docs if doc.content_tokens is not None)
    print(f"   âœ… Tokenè®¡ç®—å®Œæˆ: {calculated_count}/{len(docs)} ä¸ªæ–‡æ¡£")


async def _process_summaries_async(
    unique_predocs: List[PreDoc],
    api_key: str,
    base_url: str,
    chat_model: str,
    max_embedding_length: int,
    max_token_per_block: int,
    token_calculator: TokenCalculator,
    max_concurrent: int = 50
):
    """å¼‚æ­¥å¤„ç†æ‰€æœ‰æ–‡æ¡£çš„æ‘˜è¦ç”Ÿæˆï¼Œå¸¦å¹¶å‘æ§åˆ¶å’Œå†…å®¹å»é‡"""
    print(f"\nğŸ“ å¼€å§‹å¤„ç† {len(unique_predocs)} ä¸ªæ–‡æ¡£çš„æ‘˜è¦ç”Ÿæˆ...")
    print(f"ğŸ”§ é…ç½®: æœ€å¤§å¹¶å‘={max_concurrent}, æ¨¡å‹={chat_model}")
    
    semaphore = asyncio.Semaphore(max_concurrent)
    
    # ç¬¬ä¸€æ­¥ï¼šæ”¶é›†æ‰€æœ‰éœ€è¦ç”Ÿæˆæ‘˜è¦çš„å†…å®¹ï¼Œå¹¶è¿›è¡Œå»é‡
    print("ğŸ” ç¬¬ä¸€æ­¥: åˆ†ææ–‡æ¡£å†…å®¹å¹¶è¿›è¡Œå»é‡...")
    print("   ğŸ“Š æ­£åœ¨è®¡ç®—tokenæ•°é‡å’Œåˆ†å—...")
    
    content_to_docs = {}  # content_hash -> list of docs
    content_to_blocks = {}  # content_hash -> list of (doc, block_index, block_content)
    
    # ç»Ÿè®¡å˜é‡
    docs_need_summary = [doc for doc in unique_predocs if not doc.summary]
    total_docs_to_analyze = len(docs_need_summary)
    analyzed_count = 0
    long_docs_count = 0
    total_blocks_created = 0
    
    for doc in docs_need_summary:
        analyzed_count += 1
        content = doc.content
        
        # æ˜¾ç¤ºtokenè®¡ç®—è¿›åº¦
        if analyzed_count % 10 == 0 or analyzed_count == total_docs_to_analyze:
            print(f"   ğŸ”¢ [{analyzed_count}/{total_docs_to_analyze}] Tokenè®¡ç®—è¿›åº¦: {(analyzed_count/total_docs_to_analyze*100):.1f}%")
        
        token_count = token_calculator.count_tokens(content)
        
        if token_count <= max_token_per_block:
            # å†…å®¹ä¸è¶…é™ï¼Œç›´æ¥ç”Ÿæˆæ‘˜è¦
            content_hash = hash(content)
            if content_hash not in content_to_docs:
                content_to_docs[content_hash] = []
            content_to_docs[content_hash].append(doc)
        else:
            # å†…å®¹è¶…é™ï¼Œåˆ†å—å¤„ç†
            long_docs_count += 1
            blocks = []
            start = 0
            content_len = len(content)
            
            print(f"   ğŸ“„ æ–‡æ¡£è¿‡é•¿éœ€åˆ†å—: {doc.source[:50] if doc.source else 'unknown'}... (tokens: {token_count})")
            # å¦‚æœtokenè¶…è¿‡25wå°±ä¸¢å¼ƒ
            if token_count > 25 * 10000:
                continue
            # ä½¿ç”¨é«˜æ€§èƒ½åˆ†å—å™¨
            from .fast_token_splitter import FastTokenSplitter
            splitter = FastTokenSplitter(
                token_calculator=token_calculator,
                chunk_size=max_token_per_block,
                chunk_overlap=50  # å°çš„é‡å ä»¥ä¿æŒä¸Šä¸‹æ–‡
            )
            blocks = splitter.split_text(content)
            
            total_blocks_created += len(blocks)
            print(f"   âœ‚ï¸  åˆ†å—å®Œæˆ: åˆ›å»ºäº† {len(blocks)} ä¸ªå—")
            
            # å¯¹æ¯ä¸ªå—è¿›è¡Œå»é‡
            for block_index, block in enumerate(blocks):
                block_hash = hash(block)
                if block_hash not in content_to_blocks:
                    content_to_blocks[block_hash] = []
                content_to_blocks[block_hash].append((doc, block_index, block))
    
    print(f"   ğŸ“ˆ Tokenåˆ†æå®Œæˆç»Ÿè®¡:")
    print(f"      - åˆ†ææ–‡æ¡£æ€»æ•°: {total_docs_to_analyze}")
    print(f"      - éœ€è¦åˆ†å—çš„é•¿æ–‡æ¡£: {long_docs_count}")
    print(f"      - åˆ›å»ºçš„æ€»å—æ•°: {total_blocks_created}")
    print(f"      - å¹³å‡æ¯ä¸ªé•¿æ–‡æ¡£åˆ†å—: {(total_blocks_created/long_docs_count):.1f}" if long_docs_count > 0 else "      - å¹³å‡æ¯ä¸ªé•¿æ–‡æ¡£åˆ†å—: 0")
    
    # ç»Ÿè®¡å»é‡æ•ˆæœ
    total_docs = len([doc for doc in unique_predocs if not doc.summary])
    unique_contents = len(content_to_docs)
    unique_blocks = len(content_to_blocks)
    total_unique_tasks = unique_contents + unique_blocks
    
    print(f"ğŸ“Š å»é‡ç»Ÿè®¡:")
    print(f"   - éœ€è¦å¤„ç†çš„æ–‡æ¡£: {total_docs}")
    print(f"   - å»é‡åçš„å®Œæ•´å†…å®¹: {unique_contents}")
    print(f"   - å»é‡åçš„åˆ†å—å†…å®¹: {unique_blocks}")
    print(f"   - æ€»è®¡éœ€è¦ç”Ÿæˆæ‘˜è¦: {total_unique_tasks}")
    if total_docs > 0:
        print(f"   - å»é‡æ•ˆç‡: {((total_docs - total_unique_tasks) / total_docs * 100):.1f}%")
    
    # ç¬¬äºŒæ­¥ï¼šä¸ºå»é‡åçš„å†…å®¹åˆ›å»ºå¼‚æ­¥ä»»åŠ¡
    print("ğŸš€ ç¬¬äºŒæ­¥: åˆ›å»ºå¼‚æ­¥ä»»åŠ¡...")
    
    # åˆå§‹åŒ–è¿›åº¦è®¡æ•°å™¨
    progress_counter = {
        'current': 0,
        'total': total_unique_tasks,
        'completed': 0,
        'failed': 0
    }
    
    unique_content_tasks = {}  # content_hash -> task
    unique_block_tasks = {}   # block_hash -> task
    
    # å¤„ç†å®Œæ•´æ–‡æ¡£å†…å®¹
    for content_hash, docs in content_to_docs.items():
        content = docs[0].content  # æ‰€æœ‰docçš„contentéƒ½ç›¸åŒï¼Œå–ç¬¬ä¸€ä¸ª
        # ç”Ÿæˆä»»åŠ¡IDç”¨äºè¿›åº¦æ˜¾ç¤º
        task_id = f"å®Œæ•´æ–‡æ¡£-{docs[0].source if docs[0].source else 'unknown'}"
        task = _generate_summary_async(
            content, api_key, base_url, chat_model, semaphore, progress_counter, task_id
        )
        unique_content_tasks[content_hash] = task
    
    # å¤„ç†åˆ†å—å†…å®¹
    for block_hash, block_infos in content_to_blocks.items():
        block_content = block_infos[0][2]  # æ‰€æœ‰blockå†…å®¹éƒ½ç›¸åŒï¼Œå–ç¬¬ä¸€ä¸ª
        doc, block_index, _ = block_infos[0]
        # ç”Ÿæˆä»»åŠ¡IDç”¨äºè¿›åº¦æ˜¾ç¤º
        task_id = f"åˆ†å—{block_index}-{doc.source if doc.source else 'unknown'}"
        task = _generate_summary_async(
            block_content, api_key, base_url, chat_model, semaphore, progress_counter, task_id
        )
        unique_block_tasks[block_hash] = task
    
    # ç¬¬ä¸‰æ­¥ï¼šæ‰§è¡Œæ‰€æœ‰å»é‡åçš„ä»»åŠ¡
    print(f"âš¡ ç¬¬ä¸‰æ­¥: å¹¶å‘æ‰§è¡Œ {total_unique_tasks} ä¸ªæ‘˜è¦ç”Ÿæˆä»»åŠ¡...")
    print(f"   ğŸ“Š å®æ—¶è¿›åº¦æ˜¾ç¤º:")
    
    try:
        # åˆå¹¶æ‰€æœ‰ä»»åŠ¡åˆ°ä¸€ä¸ªåˆ—è¡¨ä¸­ï¼Œä»¥ä¾¿ç»Ÿä¸€å¤„ç†è¿›åº¦
        all_tasks = []
        task_to_hash = {}
        
        # æ·»åŠ å®Œæ•´æ–‡æ¡£ä»»åŠ¡
        for content_hash, task in unique_content_tasks.items():
            all_tasks.append(task)
            task_to_hash[task] = ('content', content_hash)
        
        # æ·»åŠ åˆ†å—ä»»åŠ¡
        for block_hash, task in unique_block_tasks.items():
            all_tasks.append(task)
            task_to_hash[task] = ('block', block_hash)
        
        # å¹¶å‘æ‰§è¡Œæ‰€æœ‰ä»»åŠ¡
        if all_tasks:
            all_results = await asyncio.gather(*all_tasks, return_exceptions=True)
            
            # å°†ç»“æœåˆ†é…å›å¯¹åº”çš„å­—å…¸
            content_hash_to_result = {}
            block_hash_to_result = {}
            
            for task, result in zip(all_tasks, all_results):
                task_type, task_hash = task_to_hash[task]
                if task_type == 'content':
                    content_hash_to_result[task_hash] = result
                else:  # block
                    block_hash_to_result[task_hash] = result
        else:
            content_hash_to_result = {}
            block_hash_to_result = {}
        
        # æ‰“å°æœ€ç»ˆè¿›åº¦ç»Ÿè®¡
        print(f"   ğŸ“ˆ ä»»åŠ¡æ‰§è¡Œå®Œæˆ:")
        print(f"      - å·²å®Œæˆ: {progress_counter['completed']}")
        print(f"      - å·²å¤±è´¥: {progress_counter['failed']}")
        print(f"      - æ€»è®¡: {progress_counter['total']}")
        
        # ç¬¬å››æ­¥ï¼šå°†ç»“æœåˆ†é…ç»™ç›¸åº”çš„æ–‡æ¡£
        print("ğŸ“‹ ç¬¬å››æ­¥: åˆ†é…æ‘˜è¦ç»“æœåˆ°æ–‡æ¡£...")
        
        # ç»Ÿè®¡æˆåŠŸå’Œå¤±è´¥çš„ä»»åŠ¡
        success_count = 0
        error_count = 0
        
        # å¤„ç†å®Œæ•´æ–‡æ¡£
        for content_hash, docs in content_to_docs.items():
            result = content_hash_to_result.get(content_hash)
            if isinstance(result, Exception):
                summary = docs[0].content[:max_embedding_length]
                error_count += 1
            else:
                summary = result[:max_embedding_length] if result else docs[0].content[:max_embedding_length]
                success_count += 1
            
            # å°†ç›¸åŒå†…å®¹çš„æ‘˜è¦åˆ†é…ç»™æ‰€æœ‰ç›¸å…³æ–‡æ¡£
            for doc in docs:
                doc.summary = summary
        
        # å¤„ç†åˆ†å—æ–‡æ¡£
        doc_block_summaries = {}  # doc -> {block_index: summary}
        for block_hash, block_infos in content_to_blocks.items():
            result = block_hash_to_result.get(block_hash)
            if isinstance(result, Exception):
                summary = block_infos[0][2][:max_embedding_length]  # ä½¿ç”¨åŸå§‹å—å†…å®¹
                error_count += 1
            else:
                summary = result if result else block_infos[0][2][:max_embedding_length]
                success_count += 1
            
            # å°†ç›¸åŒå—å†…å®¹çš„æ‘˜è¦åˆ†é…ç»™æ‰€æœ‰ç›¸å…³æ–‡æ¡£çš„å¯¹åº”å—
            for doc, block_index, block_content in block_infos:
                if doc not in doc_block_summaries:
                    doc_block_summaries[doc] = {}
                doc_block_summaries[doc][block_index] = summary
        
        # åˆå¹¶æ¯ä¸ªæ–‡æ¡£çš„æ‰€æœ‰å—æ‘˜è¦
        for doc, block_summaries in doc_block_summaries.items():
            # æŒ‰å—ç´¢å¼•æ’åºå¹¶åˆå¹¶
            sorted_summaries = [block_summaries[i] for i in sorted(block_summaries.keys())]
            doc.summary = '\n'.join(sorted_summaries)[:max_embedding_length]
        
        # æ‰“å°æœ€ç»ˆç»Ÿè®¡
        print(f"ğŸ“ˆ æ‘˜è¦ç”Ÿæˆå®Œæˆç»Ÿè®¡:")
        print(f"   - æˆåŠŸç”Ÿæˆ: {success_count}")
        print(f"   - å¤±è´¥å›é€€: {error_count}")
        print(f"   - æˆåŠŸç‡: {(success_count / (success_count + error_count) * 100):.1f}%" if (success_count + error_count) > 0 else "   - æˆåŠŸç‡: 100%")
            
    except Exception as e:
        print(f"âŒ æ‘˜è¦ç”Ÿæˆè¿‡ç¨‹å‡ºç°å¼‚å¸¸: {e}")
        # å¦‚æœæ•´ä¸ªè¿‡ç¨‹å¤±è´¥ï¼Œä¸ºæ‰€æœ‰æœªå¤„ç†çš„æ–‡æ¡£ä½¿ç”¨åŸå§‹å†…å®¹
        for doc in unique_predocs:
            if not doc.summary:
                doc.summary = doc.content[:max_embedding_length]


def process_and_add_documents(
    docs: List[PreDoc],
    content_pool,
    url_hash_set: Set[int],
    raw_content_hash_set: Set[int],
    api_key: str,
    base_url: str,
    chat_model: str,
    chat_max_token_length: int,  # Chatæ¨¡å‹çš„æœ€å¤§tokené•¿åº¦ï¼Œç”¨äºåˆ†å—
    embedding_model: str,
    embedding_model_api_key: str,
    embedding_model_base_url: str,
    max_embedding_length: int,  # ç”¨äºæ‘˜è¦æˆªæ–­ï¼Œä¸æ˜¯åˆ†å—
    get_text_embedding_fn,
    max_concurrent: int = 50
) -> Tuple[Set[int], Set[int]]:
    """
    å¤„ç†å¹¶æ·»åŠ æ–‡æ¡£åˆ°å†…å®¹æ± ï¼Œè¿”å›æ›´æ–°åçš„å»é‡å“ˆå¸Œé›†ã€‚
    è¿™æ˜¯ä¸€ä¸ªçº¯å‡½æ•°ï¼Œä¸ä¿®æ”¹ç±»çŠ¶æ€ï¼Œåªæ“ä½œä¼ å…¥çš„å‚æ•°ã€‚
    éœ€è¦å¤–éƒ¨ä¼ å…¥ chat_no_tool_fn, get_text_embedding_fnã€‚
    """
    print(f"\nğŸš€ å¼€å§‹å¤„ç†æ–‡æ¡£å¯¼å…¥æµç¨‹...")
    print(f"ğŸ“¥ è¾“å…¥æ–‡æ¡£æ•°é‡: {len(docs)}")
    
    # 1. å»é‡
    print("ğŸ” æ­¥éª¤1: æ–‡æ¡£å»é‡...")
    unique_predocs = []
    duplicate_count = 0
    
    for doc in docs:
        url_hash = hash(doc.source)
        raw_content_hash = hash(doc.raw_content)
        if url_hash in url_hash_set or raw_content_hash in raw_content_hash_set:
            duplicate_count += 1
            continue
        url_hash_set.add(url_hash)
        raw_content_hash_set.add(raw_content_hash)
        unique_predocs.append(doc)
    
    print(f"   - å»é‡åæ–‡æ¡£æ•°é‡: {len(unique_predocs)}")
    print(f"   - é‡å¤æ–‡æ¡£æ•°é‡: {duplicate_count}")
    
    if not unique_predocs:
        print("âš ï¸  æ²¡æœ‰æ–°æ–‡æ¡£éœ€è¦å¤„ç†ï¼Œè·³è¿‡åç»­æ­¥éª¤")
        return url_hash_set, raw_content_hash_set

    # é»˜è®¤ç”¨ OpenAITokenCalculatorï¼Œå¯æ ¹æ®å®é™…æƒ…å†µæ›¿æ¢
    token_calculator = OpenAITokenCalculator()
    # ä½¿ç”¨chatæ¨¡å‹çš„tokené™åˆ¶æ¥è®¡ç®—åˆ†å—å¤§å°ï¼Œé¢„ç•™ä¸€äº›ç©ºé—´ç»™prompt
    max_token_per_block = int(chat_max_token_length * 0.6)  # ä½¿ç”¨chat tokené™åˆ¶çš„60%ä½œä¸ºåˆ†å—å¤§å°
    
    print(f"ğŸ”§ åˆ†å—é…ç½®:")
    print(f"   - Chatæ¨¡å‹æœ€å¤§token: {chat_max_token_length}")
    print(f"   - æ¯å—æœ€å¤§token: {max_token_per_block}")
    print(f"   - æ‘˜è¦æˆªæ–­é•¿åº¦: {max_embedding_length}")

    # 2. å¼‚æ­¥ç”Ÿæˆæ‘˜è¦ï¼ˆä½¿ç”¨å¹¶å‘æ§åˆ¶ï¼‰
    asyncio.run(_process_summaries_async(
        unique_predocs=unique_predocs,
        api_key=api_key,
        base_url=base_url,
        chat_model=chat_model,
        max_embedding_length=max_embedding_length,
        max_token_per_block=max_token_per_block,
        token_calculator=token_calculator,
        max_concurrent=max_concurrent
    ))

    # 3. è‡ªåŠ¨Embedding
    print("ğŸ”— æ­¥éª¤3: ç”Ÿæˆå‘é‡åµŒå…¥...")
    docs_to_embed = [doc.summary for doc in unique_predocs if doc.vector is None]
    print(f"   - éœ€è¦ç”Ÿæˆå‘é‡çš„æ–‡æ¡£: {len(docs_to_embed)}")
    
    if docs_to_embed:
        print(f"   - ä½¿ç”¨æ¨¡å‹: {embedding_model}")
        vectors = get_text_embedding_fn(
            docs_to_embed,
            api_key=embedding_model_api_key,
            base_url=embedding_model_base_url,
            embedding_model=embedding_model,
        )
        print(f"   - ç”Ÿæˆå‘é‡æ•°é‡: {len(vectors)}")
        
        # å°†ç”Ÿæˆçš„å‘é‡èµ‹å›ç»™ç›¸åº”çš„æ–‡æ¡£
        vec_idx = 0
        for doc in unique_predocs:
            if doc.vector is None:
                doc.vector = vectors[vec_idx]
                vec_idx += 1
        print("   âœ… å‘é‡åµŒå…¥å®Œæˆ")
    else:
        print("   âš ï¸  æ‰€æœ‰æ–‡æ¡£å·²æœ‰å‘é‡ï¼Œè·³è¿‡å‘é‡ç”Ÿæˆ")

    # 4. æ ¼å¼åŒ–å¹¶å…¥åº“
    print("ğŸ’¾ æ­¥éª¤4: æ ¼å¼åŒ–å¹¶å…¥åº“...")
    doc_objs = []
    for doc in unique_predocs:
        if doc.vector is not None:
            payload = {
                "content": doc.content,
                "source": doc.source,
                "summary": doc.summary,
                "data_source_type": doc.data_source_type,
                **doc.others
            }
            # id ä¼ é€’åˆ° Doc å¯¹è±¡å’Œ payload
            doc_objs.append(Doc(id=doc.id, vector=doc.vector, payload=payload))
    
    print(f"   - å‡†å¤‡å…¥åº“çš„æ–‡æ¡£: {len(doc_objs)}")
    if doc_objs:
        content_pool.insert_contents([asdict(d) for d in doc_objs])
        print("   âœ… æ–‡æ¡£å…¥åº“å®Œæˆ")
    else:
        print("   âš ï¸  æ²¡æœ‰æ–‡æ¡£éœ€è¦å…¥åº“")

    print(f"ğŸ‰ æ–‡æ¡£å¯¼å…¥æµç¨‹å®Œæˆï¼")
    print(f"   - æ€»å¤„ç†æ–‡æ¡£: {len(unique_predocs)}")
    print(f"   - æˆåŠŸå…¥åº“: {len(doc_objs)}")
    
    return url_hash_set, raw_content_hash_set
