"""
åŸºç¡€æ•°æ®æ”¶é›†ç±»
æä¾›é€šç”¨çš„æ•°æ®æ”¶é›†æµç¨‹ï¼ŒåŒ…æ‹¬å¤§çº²ç”Ÿæˆã€æœç´¢æŸ¥è¯¢ã€æ•°æ®æ”¶é›†ã€æ‘˜è¦ã€åˆ†é…å’Œå¯è§†åŒ–ç­‰åŠŸèƒ½
"""
import os
import json
import time
import traceback
from abc import ABC, abstractmethod
from typing import Dict, List, Any, Optional
from dotenv import load_dotenv
from financial_report.search_tools.search_tools import bing_search_with_cache, zhipu_search_with_cache
from data_process.content_summarizer import generate_summaries_for_collected_data
from data_process.outline_data_allocator import allocate_data_to_outline_sync
from data_process.search_data_processor import SearchDataProcessor


class BaseDataCollection(ABC):
    """åŸºç¡€æ•°æ®æ”¶é›†ç±»"""
    
    def __init__(self, target_name: str, data_type: str, max_concurrent: int = 190, 
                 api_key: Optional[str] = None, base_url: Optional[str] = None, 
                 model: Optional[str] = None, use_zhipu_search: bool = False, zhipu_search_key: str = None,
                 search_url: Optional[str] = None, search_interval: float = 1.0, 
                 use_existing_search_results: bool = True):
        """
        åˆå§‹åŒ–åŸºç¡€æ•°æ®æ”¶é›†å™¨
        
        Args:
            target_name: ç›®æ ‡åç§°ï¼ˆè¡Œä¸šåã€å®è§‚ä¸»é¢˜ç­‰ï¼‰
            data_type: æ•°æ®ç±»å‹ï¼ˆindustryã€macroç­‰ï¼‰
            max_concurrent: æœ€å¤§å¹¶å‘æ•°
            api_key: APIå¯†é’¥ï¼Œå¦‚æœä¸ºNoneåˆ™ä»ç¯å¢ƒå˜é‡è·å–
            base_url: APIåŸºç¡€URLï¼Œå¦‚æœä¸ºNoneåˆ™ä»ç¯å¢ƒå˜é‡è·å–
            model: æ¨¡å‹åç§°ï¼Œå¦‚æœä¸ºNoneåˆ™ä»ç¯å¢ƒå˜é‡è·å–
            use_zhipu_search: æ˜¯å¦ä½¿ç”¨æ™ºè°±æœç´¢ï¼Œé»˜è®¤Falseä½¿ç”¨æœ¬åœ°æœç´¢æœåŠ¡
            zhipu_search_key: æ™ºè°±æœç´¢APIå¯†é’¥
            search_url: æœ¬åœ°æœç´¢æœåŠ¡URLï¼Œå¦‚æœä¸ºNoneåˆ™ä»ç¯å¢ƒå˜é‡è·å–
            search_interval: æœç´¢é—´éš”æ—¶é—´ï¼ˆç§’ï¼‰ï¼Œé»˜è®¤1.0ç§’ï¼Œé˜²æ­¢è¯·æ±‚è¿‡äºé¢‘ç¹
            use_existing_search_results: æ˜¯å¦ä½¿ç”¨å·²æœ‰æœç´¢ç»“æœï¼Œé»˜è®¤Trueï¼ŒèŠ‚çœæœç´¢æˆæœ¬
        """
        self.target_name = target_name
        self.data_type = data_type
        self.max_concurrent = max_concurrent
        self.use_zhipu_search = use_zhipu_search
        self.search_interval = search_interval
        self.use_existing_search_results = use_existing_search_results
        
        # åŠ è½½ç¯å¢ƒå˜é‡
        load_dotenv()
        self._setup_api_config(api_key, base_url, model, zhipu_search_key=zhipu_search_key, search_url=search_url)
        self._setup_paths()
        
    def _setup_api_config(self, api_key: Optional[str] = None, base_url: Optional[str] = None, 
                         model: Optional[str] = None, zhipu_search_key: Optional[str] = None,
                         search_url: Optional[str] = None):
        """è®¾ç½®APIé…ç½®"""
        # Chat éƒ¨åˆ†ç»Ÿä¸€ä½¿ç”¨é€šç”¨çš„APIé…ç½®ï¼ˆç¡…åŸºæµåŠ¨ç­‰ï¼‰
        self.api_key = api_key or os.getenv("GUIJI_API_KEY")
        self.base_url = base_url or os.getenv("GUIJI_BASE_URL")
        self.model = model or os.getenv("GUIJI_TEXT_MODEL_DEEPSEEK_PRO")
        self.max_chat_tokens = int(128 * 1024 * 0.8)
        self.search_url = search_url or os.getenv("SEARCH_URL")
        
        # æœç´¢éƒ¨åˆ†ä¸“é—¨çš„æ™ºè°±é…ç½®
        if self.use_zhipu_search and not zhipu_search_key:
            raise ValueError("zhipu API key is required for using Zhipu search.")

        self.zhipu_api_key = zhipu_search_key
        self.zhipu_base_url = os.getenv("ZHIPU_BASE_URL")
        self.zhipu_model = os.getenv("ZHIPU_FREE_TEXT_MODEL") 
        self.zhipu_max_chat_tokens = int(128 * 1024 * 0.8)
        
    def _setup_paths(self):
        """è®¾ç½®æ–‡ä»¶è·¯å¾„"""
        # åˆ›å»ºæ•°æ®ç›®å½•
        self.data_dir = f"test_{self.data_type}_datas"
        if not os.path.exists(self.data_dir):
            os.mkdir(self.data_dir)
            
        # å®šä¹‰æ–‡ä»¶è·¯å¾„
        self.outline_file = os.path.join(self.data_dir, f"{self.data_type}_outline.json")
        self.flattened_data_file = os.path.join(self.data_dir, f"flattened_{self.data_type}_data.json")
        self.allocation_result_file = os.path.join(self.data_dir, "outline_data_allocation.json")
        self.viz_results_file = os.path.join(self.data_dir, "visualization_data_results.json")
        
        # å¯è§†åŒ–è·¯å¾„é…ç½®
        self.visualization_html_output_dir = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))  # é¡¹ç›®æ ¹ç›®å½•
        self.visualization_assets_output_dir = os.path.join(self.data_dir, "images")
        
        if not os.path.exists(self.visualization_assets_output_dir):
            os.makedirs(self.visualization_assets_output_dir, exist_ok=True)
    
    @abstractmethod
    def generate_outline(self) -> Dict[str, Any]:
        """ç”Ÿæˆå¤§çº² - å­ç±»å¿…é¡»å®ç°"""
        pass
    
    @abstractmethod
    def generate_search_queries(self, outline_result: Dict[str, Any]) -> List[Any]:
        """ç”Ÿæˆæœç´¢æŸ¥è¯¢ - å­ç±»å¿…é¡»å®ç°"""
        pass
    
    @abstractmethod
    def create_visual_enhancer(self):
        """åˆ›å»ºå¯è§†åŒ–æ•°æ®å¢å¼ºå™¨ - å­ç±»å¿…é¡»å®ç°"""
        pass
    
    @abstractmethod
    def create_visualization_processor(self):
        """åˆ›å»ºå¯è§†åŒ–æ•°æ®å¤„ç†å™¨ - å­ç±»å¿…é¡»å®ç°"""
        pass
    
    def print_start_banner(self):
        """æ‰“å°å¼€å§‹æ¨ªå¹…"""
        search_type = "æ™ºè°±æœç´¢" if self.use_zhipu_search else "Bingæœç´¢"
        existing_results_info = "ä½¿ç”¨å·²æœ‰ç»“æœ" if self.use_existing_search_results else "é‡æ–°æœç´¢"
        print("=" * 60)
        print(f"ğŸš€ å¯åŠ¨{self.data_type}ç ”ç©¶æŠ¥å‘Šæ•°æ®æ”¶é›†æµç¨‹")
        print(f"ğŸ¯ ç›®æ ‡{self.data_type}: {self.target_name}")
        print(f"ğŸ” æœç´¢æ–¹å¼: {search_type}")
        print(f"ğŸ“ æœç´¢ç­–ç•¥: {existing_results_info}")
        print(f"â±ï¸ æœç´¢é—´éš”: {self.search_interval}ç§’")
        print("=" * 60)
    
    def step1_generate_outline(self) -> Dict[str, Any]:
        """æ­¥éª¤1: ç”Ÿæˆå¤§çº²"""
        print(f"\næ­¥éª¤ 1ï¼šç”Ÿæˆ{self.data_type}å¤§çº²")
        print("="*50)
        
        try:
            outline_result = self.generate_outline()
            
            if outline_result:
                # ç¡®ä¿æœ‰æ ‡å‡†æ ¼å¼ - ç»Ÿä¸€ä½¿ç”¨å…¬å¸æ ¼å¼
                if "reportOutline" not in outline_result:
                    outline_result = {
                        "reportOutline": outline_result.get("outline", []),
                        f"{self.data_type}Name": outline_result.get(f"{self.data_type}Name", self.target_name)
                    }
                    
                    # ä¸ºä¸åŒç±»å‹æ·»åŠ ç‰¹å®šå­—æ®µ
                    if self.data_type == "company":
                        outline_result["companyCode"] = outline_result.get("companyCode", "")
                    elif self.data_type == "industry":
                        # è¡Œä¸šç±»å‹ä¸éœ€è¦ä»£ç å­—æ®µ
                        pass
                    elif self.data_type == "macro":
                        # å®è§‚ç±»å‹ä¸éœ€è¦ä»£ç å­—æ®µ  
                        pass
            else:
                outline_result = {"reportOutline": []}
            
            with open(self.outline_file, "w", encoding="utf-8") as f:
                json.dump(outline_result, f, ensure_ascii=False, indent=2)
            
            print(f"âœ… {self.data_type}å¤§çº²ç”Ÿæˆå®Œæˆï¼Œå…± {len(outline_result.get('reportOutline', []))} ä¸ªç« èŠ‚")
            return outline_result
            
        except Exception as e:
            print(f"âŒ {self.data_type}å¤§çº²ç”Ÿæˆå¤±è´¥: {e}")
            return {"outline": []}
    
    def step2_collect_data(self, outline_result: Dict[str, Any]) -> List[Dict[str, Any]]:
        """æ­¥éª¤2: ç”Ÿæˆæœç´¢æŸ¥è¯¢å¹¶æ”¶é›†æ•°æ®"""
        print(f"\næ­¥éª¤ 2ï¼šç”Ÿæˆæœç´¢æŸ¥è¯¢å¹¶æ”¶é›†æ•°æ®")
        print("="*50)
        
        # æ£€æŸ¥æ˜¯å¦ä½¿ç”¨å·²æœ‰æœç´¢ç»“æœ
        if self.use_existing_search_results and os.path.exists(self.flattened_data_file):
            try:
                with open(self.flattened_data_file, "r", encoding="utf-8") as f:
                    existing_data = json.load(f)
                
                if existing_data and len(existing_data) > 0:
                    print(f"ğŸ“ å‘ç°å·²æœ‰æœç´¢ç»“æœæ–‡ä»¶: {self.flattened_data_file}")
                    print(f"ğŸ“Š å·²æœ‰æ•°æ®é¡¹æ•°é‡: {len(existing_data)}")
                    print(f"ğŸ’° ä½¿ç”¨å·²æœ‰æœç´¢ç»“æœï¼ŒèŠ‚çœæœç´¢æˆæœ¬")
                    print(f"âœ… è·³è¿‡æœç´¢æ­¥éª¤ï¼Œç›´æ¥ä½¿ç”¨å·²æœ‰çš„ {len(existing_data)} æ¡æ•°æ®")
                    return existing_data
                else:
                    print(f"ğŸ“ æœç´¢ç»“æœæ–‡ä»¶å­˜åœ¨ä½†ä¸ºç©ºï¼Œå°†é‡æ–°æœç´¢")
            except Exception as e:
                print(f"âš ï¸ è¯»å–å·²æœ‰æœç´¢ç»“æœå¤±è´¥: {e}")
                print(f"ğŸ”„ å°†é‡æ–°æ‰§è¡Œæœç´¢")
        elif self.use_existing_search_results:
            print(f"ğŸ“ æœç´¢ç»“æœæ–‡ä»¶ä¸å­˜åœ¨: {self.flattened_data_file}")
            print(f"ğŸ”„ å°†æ‰§è¡Œæ–°çš„æœç´¢")
        else:
            print(f"ğŸ”„ é…ç½®ä¸ºé‡æ–°æœç´¢ï¼Œå¿½ç•¥å·²æœ‰ç»“æœæ–‡ä»¶")
        
        try:
            # ç”Ÿæˆæœç´¢æŸ¥è¯¢
            queries_list = self.generate_search_queries(outline_result)
            print(f"âœ… ç”Ÿæˆ {len(queries_list)} ä¸ªæœç´¢æŸ¥è¯¢")
            
            # åˆ›å»ºæœç´¢æ•°æ®å¤„ç†å™¨
            search_processor = SearchDataProcessor(
                api_key=self.api_key,
                base_url=self.base_url,
                model=self.model,
                summary_api_key=self.api_key,
                summary_base_url=self.base_url,
                summary_model=self.model
            )
            
            # ç¬¬ä¸€é˜¶æ®µï¼šæ‰¹é‡æ”¶é›†æ‰€æœ‰æœç´¢æ•°æ®
            print("\nğŸ” ç¬¬ä¸€é˜¶æ®µï¼šæ‰¹é‡æ”¶é›†æœç´¢æ•°æ®...")
            search_start_time = time.time()
            all_raw_search_results = []
            current_id = 1
            
            for i, query_info in enumerate(queries_list, 1):
                query = query_info.get("query", query_info) if isinstance(query_info, dict) else query_info
                section = query_info.get("section_title", f"{self.data_type}ç ”ç©¶") if isinstance(query_info, dict) else f"{self.data_type}ç ”ç©¶"
                
                print(f"ğŸ“Š [{i}/{len(queries_list)}] æœç´¢: {query[:60]}...")
                
                # æ·»åŠ æœç´¢é—´éš”æ§åˆ¶ï¼Œé˜²æ­¢è¯·æ±‚è¿‡äºé¢‘ç¹
                if i > 1:  # ç¬¬ä¸€ä¸ªè¯·æ±‚ä¸éœ€è¦ç­‰å¾…
                    print(f"â³ ç­‰å¾… {self.search_interval} ç§’ï¼Œé˜²æ­¢è¯·æ±‚è¿‡äºé¢‘ç¹...")
                    time.sleep(self.search_interval)
                
                try:
                    # æ ¹æ®é…ç½®é€‰æ‹©æœç´¢æ–¹å¼
                    if self.use_zhipu_search:
                        print(f"ğŸ” ä½¿ç”¨æ™ºè°±æœç´¢...")
                        search_results = zhipu_search_with_cache(
                            query=query,
                            count=50,
                            force_refresh=False,
                            zhipu_api_key=self.zhipu_api_key,
                            timeout=30,
                            rate_limit_delay=0.5
                        )
                    else:
                        print(f"ğŸ” ä½¿ç”¨Bingæœç´¢...")
                        search_results = bing_search_with_cache(
                            query=query, 
                            search_api_url=self.search_url,
                            total=10,
                            rate_limit_delay=0.5
                        )
                    
                    if search_results:
                        # æ ¼å¼åŒ–æœç´¢ç»“æœ
                        formatted_results = search_processor.format_search_results_to_flattened_data(
                            search_results=search_results,
                            company_name=self.target_name,
                            search_query=query,
                            start_id=current_id
                        )
                        
                        if formatted_results:
                            # æ·»åŠ ç« èŠ‚ä¿¡æ¯
                            for result in formatted_results:
                                result["section_title"] = section
                            
                            all_raw_search_results.extend(formatted_results)
                            current_id = max([int(item["id"]) for item in formatted_results]) + 1
                            print(f"   âœ… è·å¾— {len(formatted_results)} ä¸ªåŸå§‹æ•°æ®é¡¹")
                    else:
                        print(f"   âš ï¸ æœç´¢æœªè¿”å›ç»“æœ")
                
                except Exception as e:
                    print(f"   âŒ æœç´¢å¤±è´¥: {e}")
                    # æœç´¢å¤±è´¥æ—¶ä¹Ÿè¦ç­‰å¾…ï¼Œé¿å…è¿ç»­å¤±è´¥è¯·æ±‚
                    if i < len(queries_list):
                        print(f"â³ æœç´¢å¤±è´¥ï¼Œç­‰å¾… {self.search_interval * 2} ç§’åç»§ç»­...")
                        time.sleep(self.search_interval * 2)
                    continue
            
            search_end_time = time.time()
            search_duration = search_end_time - search_start_time
            print(f"âœ… ç¬¬ä¸€é˜¶æ®µå®Œæˆï¼Œå…±æ”¶é›† {len(all_raw_search_results)} ä¸ªåŸå§‹æ•°æ®é¡¹")
            print(f"â±ï¸ æœç´¢é˜¶æ®µè€—æ—¶: {search_duration:.2f}ç§’")
            
            # ç¬¬äºŒé˜¶æ®µï¼šæ‰¹é‡å¹¶å‘å¤„ç†å¤§æ¨¡å‹ä»»åŠ¡
            print(f"\nğŸ¤– ç¬¬äºŒé˜¶æ®µï¼šæ‰¹é‡å¹¶å‘å¤„ç†å¤§æ¨¡å‹ä»»åŠ¡ï¼ˆå¹¶å‘æ•°: {self.max_concurrent}ï¼‰...")
            llm_start_time = time.time()
            all_flattened_data = []
            
            if all_raw_search_results:
                try:
                    # æ‰¹é‡è´¨é‡è¯„ä¼°
                    print(f"ğŸ” å¼€å§‹æ‰¹é‡è´¨é‡è¯„ä¼° {len(all_raw_search_results)} ä¸ªæ•°æ®é¡¹...")
                    high_quality_results = search_processor.assess_search_results_quality(
                        search_results=[{
                            "url": item["url"],
                            "title": item["title"], 
                            "md": item["content"],
                            "data_source_type": item["data_source_type"]
                        } for item in all_raw_search_results],
                        company_name=self.target_name,
                        section_title=f"{self.data_type}ç ”ç©¶",
                        max_concurrent=self.max_concurrent
                    )
                    
                    if high_quality_results:
                        print(f"âœ… è´¨é‡è¯„ä¼°å®Œæˆï¼Œç­›é€‰å‡º {len(high_quality_results)} ä¸ªé«˜è´¨é‡æ•°æ®é¡¹")
                        
                        # å°†è´¨é‡è¯„ä¼°ç»“æœæ˜ å°„å›åŸå§‹æ•°æ®
                        high_quality_urls = {item["url"] for item in high_quality_results}
                        filtered_raw_results = []
                        
                        for raw_item in all_raw_search_results:
                            if raw_item["url"] in high_quality_urls:
                                # æ‰¾åˆ°å¯¹åº”çš„è´¨é‡è¯„ä¼°ç»“æœï¼Œæ·»åŠ è´¨é‡è¯„ä¼°ä¿¡æ¯
                                for hq_item in high_quality_results:
                                    if hq_item["url"] == raw_item["url"]:
                                        raw_item["quality_assessment"] = hq_item.get("quality_assessment", {})
                                        break
                                filtered_raw_results.append(raw_item)
                        
                        # æ‰¹é‡ç”Ÿæˆæ‘˜è¦
                        print(f"ğŸ“ å¼€å§‹æ‰¹é‡ç”Ÿæˆæ‘˜è¦...")
                        summarized_results = generate_summaries_for_collected_data(
                            data_items=filtered_raw_results,
                            api_key=self.api_key,
                            base_url=self.base_url,
                            model=self.model,
                            max_summary_length=500,
                            max_concurrent=self.max_concurrent,
                            chat_max_token_length=self.max_chat_tokens
                        )
                        
                        if summarized_results:
                            all_flattened_data = summarized_results
                            print(f"âœ… æ‘˜è¦ç”Ÿæˆå®Œæˆï¼Œæœ€ç»ˆè·å¾— {len(all_flattened_data)} ä¸ªé«˜è´¨é‡æ•°æ®é¡¹")
                        else:
                            all_flattened_data = filtered_raw_results
                            print(f"âš ï¸ æ‘˜è¦ç”Ÿæˆå¤±è´¥ï¼Œä½¿ç”¨åŸå§‹æ•°æ®: {len(all_flattened_data)} ä¸ªæ•°æ®é¡¹")
                    else:
                        print("âš ï¸ è´¨é‡è¯„ä¼°æœªç­›é€‰å‡ºé«˜è´¨é‡æ•°æ®ï¼Œä½¿ç”¨æ‰€æœ‰åŸå§‹æ•°æ®")
                        all_flattened_data = all_raw_search_results
                        
                except Exception as e:
                    print(f"âŒ å¤§æ¨¡å‹æ‰¹é‡å¤„ç†å¤±è´¥: {e}")
                    print("ğŸ“‹ å°†ä½¿ç”¨åŸå§‹æœç´¢æ•°æ®...")
                    all_flattened_data = all_raw_search_results
            else:
                print("âš ï¸ æ²¡æœ‰æ”¶é›†åˆ°æœç´¢æ•°æ®")
            
            llm_end_time = time.time()
            llm_duration = llm_end_time - llm_start_time
            total_duration = llm_end_time - search_start_time
            
            print(f"â±ï¸ å¤§æ¨¡å‹å¤„ç†è€—æ—¶: {llm_duration:.2f}ç§’")
            print(f"â±ï¸ æ€»è€—æ—¶: {total_duration:.2f}ç§’")
            print(f"ğŸ“Š æ€§èƒ½ç»Ÿè®¡: æœç´¢{search_duration:.1f}s + å¤§æ¨¡å‹{llm_duration:.1f}s = æ€»è®¡{total_duration:.1f}s")
            
            # ä¿å­˜å±•å¹³æ•°æ®
            with open(self.flattened_data_file, "w", encoding="utf-8") as f:
                json.dump(all_flattened_data, f, ensure_ascii=False, indent=2)
            
            print(f"âœ… æ•°æ®æ”¶é›†å®Œæˆï¼Œå…±è·å¾— {len(all_flattened_data)} æ¡é«˜è´¨é‡æ•°æ®")
            return all_flattened_data
            
        except Exception as e:
            print(f"âŒ æ•°æ®æ”¶é›†å¤±è´¥: {e}")
            traceback.print_exc()
            return []
    
    def step3_allocate_data(self, outline_result: Dict[str, Any], flattened_data: List[Dict[str, Any]]) -> Dict[str, Any]:
        """æ­¥éª¤3: æ•°æ®åˆ†é…åˆ°å¤§çº²"""
        print(f"\næ­¥éª¤ 3ï¼šæ•°æ®åˆ†é…åˆ°å¤§çº²")
        print("="*50)
        
        try:
            allocation_result = allocate_data_to_outline_sync(
                outline_data=outline_result,
                flattened_data=flattened_data,
                api_key=self.api_key,
                base_url=self.base_url,
                model=self.model,
                max_concurrent=self.max_concurrent
            )
            
            with open(self.allocation_result_file, "w", encoding="utf-8") as f:
                json.dump(allocation_result, f, ensure_ascii=False, indent=2)
            
            stats = allocation_result.get("allocation_stats", {})
            print(f"âœ… æ•°æ®åˆ†é…å®Œæˆ")
            print(f"   - åŒ¹é…æˆåŠŸ: {stats.get('matched_count', 0)}")
            print(f"   - åŒ¹é…ç‡: {stats.get('match_rate', 0):.1f}%")
            
            return allocation_result
            
        except Exception as e:
            print(f"âŒ æ•°æ®åˆ†é…å¤±è´¥: {e}")
            return {}
    
    def step4_visual_enhancement(self, flattened_data: List[Dict[str, Any]]) -> Optional[Dict[str, Any]]:
        """æ­¥éª¤4: å¯è§†åŒ–æ•°æ®å¢å¼º"""
        print(f"\næ­¥éª¤ 4ï¼šå¯è§†åŒ–æ•°æ®å¢å¼º")
        print("="*50)
        
        try:
            if flattened_data:
                # åˆ›å»ºå¯è§†åŒ–æ•°æ®å¢å¼ºå™¨
                visual_enhancer = self.create_visual_enhancer()
                
                # è®¾ç½®å¤§çº²æ•°æ®ï¼ˆç¡®ä¿å¯è§†åŒ–å»ºè®®ä¸å¤§çº²ç« èŠ‚åŒ¹é…ï¼‰
                if hasattr(visual_enhancer, 'set_outline_data') and os.path.exists(self.outline_file):
                    try:
                        with open(self.outline_file, "r", encoding="utf-8") as f:
                            outline_data = json.load(f)
                        visual_enhancer.set_outline_data(outline_data)
                        print(f"   ğŸ“‹ å·²è®¾ç½®å¤§çº²æ•°æ®ï¼Œç¡®ä¿ç« èŠ‚åŒ¹é…")
                    except Exception as e:
                        print(f"   âš ï¸  è®¾ç½®å¤§çº²æ•°æ®å¤±è´¥: {e}")
                
                # è¿è¡Œå¯è§†åŒ–æ•°æ®å¢å¼º
                visual_enhancement_results = visual_enhancer.run_full_enhancement_process(
                    flattened_data=flattened_data,
                    target_name=self.target_name,
                    max_concurrent=self.max_concurrent
                )
                
                analysis_phase = visual_enhancement_results.get("analysis_phase", {})
                suggestions = analysis_phase.get("visualization_suggestions", [])
                print(f"âœ… å¯è§†åŒ–æ•°æ®å¢å¼ºå®Œæˆï¼Œç”Ÿæˆ {len(suggestions)} ä¸ªå¯è§†åŒ–å»ºè®®")
                
                return visual_enhancement_results
                
            else:
                print("âš ï¸ æ²¡æœ‰å¯ç”¨æ•°æ®ï¼Œè·³è¿‡å¯è§†åŒ–å¢å¼º")
                return None
                
        except Exception as e:
            print(f"âŒ å¯è§†åŒ–æ•°æ®å¢å¼ºå¤±è´¥: {e}")
            traceback.print_exc()
            return None
    
    def step5_visualization_processing(self, visual_enhancement_results: Optional[Dict[str, Any]], 
                                     flattened_data: List[Dict[str, Any]]) -> Optional[Dict[str, Any]]:
        """æ­¥éª¤5: å¯è§†åŒ–æ•°æ®å¤„ç†"""
        print(f"\næ­¥éª¤ 5ï¼šå¯è§†åŒ–æ•°æ®å¤„ç†")
        print("="*50)
        
        try:
            if visual_enhancement_results and flattened_data:
                # åˆ›å»ºå¯è§†åŒ–æ•°æ®å¤„ç†å™¨
                viz_processor = self.create_visualization_processor()
                
                # ç›´æ¥å¤„ç†å¯è§†åŒ–æ•°æ®å¹¶ç”Ÿæˆå›¾è¡¨ï¼ˆä¸ä¾èµ–æ–‡ä»¶ï¼‰
                viz_results = viz_processor.process_visualization_data(
                    enhancement_results=visual_enhancement_results,
                    all_flattened_data=flattened_data,
                    target_name=self.target_name,
                    max_context_tokens=self.max_chat_tokens,
                    max_concurrent=self.max_concurrent
                )
                
                # ä¿å­˜å¤„ç†ç»“æœ
                with open(self.viz_results_file, "w", encoding="utf-8") as f:
                    json.dump(viz_results, f, ensure_ascii=False, indent=2)
                
                processing_summary = viz_results.get("processing_summary", {})
                successful_count = processing_summary.get("successful_count", 0)
                print(f"âœ… å¯è§†åŒ–æ•°æ®å¤„ç†å®Œæˆï¼ŒæˆåŠŸç”Ÿæˆ {successful_count} ä¸ªå›¾è¡¨")
                
                return viz_results
                
            else:
                print("âš ï¸ æ²¡æœ‰å¯è§†åŒ–å¢å¼ºç»“æœï¼Œè·³è¿‡æ•°æ®å¤„ç†")
                return None
                
        except Exception as e:
            print(f"âŒ å¯è§†åŒ–æ•°æ®å¤„ç†å¤±è´¥: {e}")
            traceback.print_exc()
            return None
    
    def print_summary(self):
        """æ‰“å°æµç¨‹æ€»ç»“"""
        print(f"\nğŸ‰ {self.data_type}æ•°æ®æ”¶é›†æµç¨‹å®Œæˆï¼")
        print("ğŸ“ ç”Ÿæˆçš„æ–‡ä»¶:")
        print(f"   - {self.data_type}å¤§çº²: {self.outline_file}")
        print(f"   - å±•å¹³æ•°æ®: {self.flattened_data_file}")
        print(f"   - æ•°æ®åˆ†é…: {self.allocation_result_file}")
        
        if os.path.exists(self.viz_results_file):
            print(f"   - å¯è§†åŒ–å¤„ç†: {self.viz_results_file}")
        
        # æ£€æŸ¥å›¾è¡¨èµ„äº§
        if os.path.exists(self.visualization_assets_output_dir):
            png_files = [f for f in os.listdir(self.visualization_assets_output_dir) if f.endswith('.png')]
            if png_files:
                print(f"   - å›¾è¡¨èµ„äº§: {len(png_files)} ä¸ªPNGæ–‡ä»¶")
    
    def run_full_process(self):
        """è¿è¡Œå®Œæ•´æµç¨‹"""
        self.print_start_banner()
        
        # æ­¥éª¤1: ç”Ÿæˆå¤§çº²
        outline_result = self.step1_generate_outline()
        
        # æ­¥éª¤2: æ”¶é›†æ•°æ®
        flattened_data = self.step2_collect_data(outline_result)
        
        # æ­¥éª¤3: æ•°æ®åˆ†é…
        allocation_result = self.step3_allocate_data(outline_result, flattened_data)
        
        # æ­¥éª¤4: å¯è§†åŒ–å¢å¼º
        visual_enhancement_results = self.step4_visual_enhancement(flattened_data)
        
        # æ­¥éª¤5: å¯è§†åŒ–å¤„ç†
        viz_results = self.step5_visualization_processing(visual_enhancement_results, flattened_data)
        
        # æ‰“å°æ€»ç»“
        self.print_summary()
        
        return {
            "outline_result": outline_result,
            "flattened_data": flattened_data,
            "allocation_result": allocation_result,
            "visual_enhancement_results": visual_enhancement_results,
            "viz_results": viz_results
        }
