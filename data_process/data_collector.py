"""
æ•°æ®æ”¶é›†åŠ©æ‰‹
ç”¨äºæ ¹æ®å¤§çº²ç« èŠ‚éœ€æ±‚æ™ºèƒ½æ”¶é›†å’Œæå–ç›¸å…³æ•°æ®ï¼Œæ”¯æŒå¤§tokené‡çš„åˆ†æ®µå¤„ç†
"""

import json
from typing import List, Dict, Any, Optional, Tuple
from financial_report.utils.calculate_tokens import OpenAITokenCalculator
from financial_report.utils.chat import chat_no_tool


class DataCollector:
    """æ•°æ®æ”¶é›†åŠ©æ‰‹ï¼Œè´Ÿè´£ä¸ºæŠ¥å‘Šç« èŠ‚æ”¶é›†ç›¸å…³æ•°æ®"""
    
    def __init__(
        self,
        api_key: str,
        base_url: str,
        model: str,
        token_calculator: Optional[OpenAITokenCalculator] = None,
        max_output_tokens: int = 16384  # å¢åŠ é»˜è®¤è¾“å‡ºtokenä»¥æ”¯æŒæ›´è¯¦ç»†çš„ä¿¡æ¯æ”¶é›†
    ):
        self.api_key = api_key
        self.base_url = base_url
        self.model = model
        self.token_calculator = token_calculator or OpenAITokenCalculator()
        self.max_output_tokens = max_output_tokens
    
    def get_data_by_ids(
        self, 
        data_ids: List[str], 
        all_data: List[Dict[str, Any]]
    ) -> List[Dict[str, Any]]:
        """
        æ ¹æ®IDåˆ—è¡¨è·å–å¯¹åº”çš„æ•°æ®é¡¹
        
        Args:
            data_ids: æ•°æ®IDåˆ—è¡¨
            all_data: æ‰€æœ‰æ•°æ®çš„åˆ—è¡¨
            
        Returns:
            åŒ¹é…çš„æ•°æ®é¡¹åˆ—è¡¨
        """
        id_to_data = {str(item["id"]): item for item in all_data}
        return [id_to_data[data_id] for data_id in data_ids if data_id in id_to_data]
    
    def calculate_content_tokens(self, data_items: List[Dict[str, Any]]) -> int:
        """
        è®¡ç®—æ•°æ®é¡¹å†…å®¹çš„æ€»tokenæ•°
        
        Args:
            data_items: æ•°æ®é¡¹åˆ—è¡¨
            
        Returns:
            æ€»tokenæ•°
        """
        total_content = ""
        for item in data_items:
            content = item.get("summary", "") or item.get("content", "") or item.get("md", "")
            total_content += f"\\n\\nã€æ•°æ®{item['id']}ã€‘{item.get('title', '')}\\n{content}"
        
        return self.token_calculator.count_tokens(total_content)
    
    def collect_data_for_section(
        self,
        section_title: str,
        section_points: List[str],
        allocated_data_ids: List[str],
        all_data: List[Dict[str, Any]],
        max_context_tokens: int,
        company_name: str = "",
        max_output_tokens: Optional[int] = None
    ) -> Dict[str, Any]:
        """
        ä¸ºç‰¹å®šç« èŠ‚æ”¶é›†æ•°æ®
        
        Args:
            section_title: ç« èŠ‚æ ‡é¢˜
            section_points: ç« èŠ‚è¦ç‚¹
            allocated_data_ids: åˆ†é…ç»™è¯¥ç« èŠ‚çš„æ•°æ®IDåˆ—è¡¨
            all_data: æ‰€æœ‰å¯ç”¨æ•°æ®
            max_context_tokens: æœ€å¤§ä¸Šä¸‹æ–‡tokenæ•°ï¼ˆå·²æ‰£é™¤promptç©ºé—´ï¼‰
            company_name: å…¬å¸åç§°
            max_output_tokens: æœ€å¤§è¾“å‡ºtokenæ•°ï¼Œå¦‚æœä¸ºNoneåˆ™ä½¿ç”¨å®ä¾‹é»˜è®¤å€¼
            
        Returns:
            åŒ…å«æ”¶é›†æ•°æ®å’Œå¤„ç†ä¿¡æ¯çš„å­—å…¸
        """
        # ç¡®å®šä½¿ç”¨çš„æœ€å¤§è¾“å‡ºtokenæ•°
        output_tokens = max_output_tokens if max_output_tokens is not None else self.max_output_tokens
        print(f"ğŸ“Š ä¸ºç« èŠ‚ '{section_title}' æ”¶é›†æ•°æ®...")
        print(f"   ğŸ“‹ åˆ†é…çš„æ•°æ®IDæ•°é‡: {len(allocated_data_ids)}")
        print(f"   ğŸ”§ æœ€å¤§è¾“å‡ºtokené…ç½®: {output_tokens:,}")
        
        # 1. è·å–åˆ†é…çš„æ•°æ®
        relevant_data = self.get_data_by_ids(allocated_data_ids, all_data)
        print(f"   âœ… æˆåŠŸè·å– {len(relevant_data)} ä¸ªæ•°æ®é¡¹")
        
        if not relevant_data:
            return {
                "section_title": section_title,
                "collected_data": [],
                "processing_method": "no_data",
                "total_tokens": 0,
                "summary": f"ç« èŠ‚ '{section_title}' æš‚æ— ç›¸å…³æ•°æ®",
                "references": []  # æ·»åŠ ç©ºçš„å‚è€ƒæ–‡çŒ®åˆ—è¡¨
            }
        
        # 2. åˆ›å»ºå‚è€ƒæ–‡çŒ®æ˜ å°„
        references = []
        id_to_ref_num = {}
        
        for i, item in enumerate(relevant_data, 1):
            # ä¼˜å…ˆä½¿ç”¨å®é™…çš„æ ‡é¢˜å’ŒURL
            actual_title = item.get("title", "")
            actual_url = item.get("url", "")
            
            # å¦‚æœæ²¡æœ‰æ ‡é¢˜ï¼Œä½¿ç”¨å…¬å¸åç§° + æ•°æ®æºç±»å‹
            if not actual_title:
                company_name = item.get("company_name", "")
                data_source_type = item.get("data_source_type", "æ•°æ®")
                actual_title = f"{company_name} {data_source_type}"
            
            ref_info = {
                "ref_num": i,
                "data_id": item["id"], 
                "title": actual_title,
                "url": actual_url,
                "source": item.get("data_source", "unknown"),
                "company_name": item.get("company_name", ""),
                "company_code": item.get("company_code", ""),
                "market": item.get("market", "")
            }
            references.append(ref_info)
            id_to_ref_num[item["id"]] = i
            
        print(f"   ğŸ“š åˆ›å»ºå‚è€ƒæ–‡çŒ®æ˜ å°„: {len(references)} ä¸ª")
        
        # 3. è®¡ç®—tokenä½¿ç”¨é‡
        total_tokens = self.calculate_content_tokens(relevant_data)
        print(f"   ğŸ“ˆ æ•°æ®æ€»tokenæ•°: {total_tokens:,}")
        print(f"   ğŸ“ å¯ç”¨ä¸Šä¸‹æ–‡token: {max_context_tokens:,}")
        
        # 4. æ ¹æ®tokené‡å†³å®šå¤„ç†æ–¹å¼
        if total_tokens <= max_context_tokens:
            # æ•°æ®é‡é€‚ä¸­ï¼Œç›´æ¥è¿”å›
            print(f"   âœ… æ•°æ®é‡é€‚ä¸­ï¼Œç›´æ¥ä½¿ç”¨å…¨éƒ¨æ•°æ®")
            return {
                "section_title": section_title,
                "section_points": section_points,
                "collected_data": relevant_data,
                "processing_method": "direct",
                "total_tokens": total_tokens,
                "company_name": company_name,
                "references": references,
                "id_to_ref_num": id_to_ref_num
            }
        else:
            # æ•°æ®é‡è¿‡å¤§ï¼Œéœ€è¦åˆ†æ‰¹å¤„ç†å’Œæå–
            print(f"   âš ï¸  æ•°æ®é‡è¿‡å¤§ï¼Œå¯ç”¨åˆ†æ‰¹æå–æ¨¡å¼")
            result = self._extract_data_in_batches(
                section_title=section_title,
                section_points=section_points,
                relevant_data=relevant_data,
                max_context_tokens=max_context_tokens,
                company_name=company_name,
                max_output_tokens=output_tokens
            )
            # æ·»åŠ å‚è€ƒæ–‡çŒ®ä¿¡æ¯
            result["references"] = references
            result["id_to_ref_num"] = id_to_ref_num
            return result
    
    def _extract_data_in_batches(
        self,
        section_title: str,
        section_points: List[str],
        relevant_data: List[Dict[str, Any]],
        max_context_tokens: int,
        company_name: str,
        max_output_tokens: int
    ) -> Dict[str, Any]:
        """
        åˆ†æ‰¹æå–æ•°æ®çš„å…³é”®ä¿¡æ¯
        
        Args:
            section_title: ç« èŠ‚æ ‡é¢˜
            section_points: ç« èŠ‚è¦ç‚¹
            relevant_data: ç›¸å…³æ•°æ®åˆ—è¡¨
            max_context_tokens: æœ€å¤§ä¸Šä¸‹æ–‡tokenæ•°
            company_name: å…¬å¸åç§°
            max_output_tokens: æœ€å¤§è¾“å‡ºtokenæ•°
            
        Returns:
            æå–åçš„æ•°æ®æ‘˜è¦
        """
        print(f"   ğŸ”„ å¼€å§‹åˆ†æ‰¹æ•°æ®æå–...")
        
        # å°†æ•°æ®åˆ†æ‰¹ï¼Œæ¯æ‰¹ä¸è¶…è¿‡tokené™åˆ¶
        batches = self._create_data_batches(relevant_data, max_context_tokens)
        print(f"   ğŸ“¦ æ•°æ®åˆ†ä¸º {len(batches)} ä¸ªæ‰¹æ¬¡å¤„ç†")
        
        # å¯¹æ¯ä¸ªæ‰¹æ¬¡è¿›è¡Œä¿¡æ¯æå–
        extracted_summaries = []
        for i, batch in enumerate(batches):
            print(f"   ğŸ”„ å¤„ç†ç¬¬ {i+1}/{len(batches)} æ‰¹æ¬¡ ({len(batch)} ä¸ªæ•°æ®é¡¹)...")
            
            try:
                batch_summary = self._extract_batch_information(
                    section_title=section_title,
                    section_points=section_points,
                    data_batch=batch,
                    company_name=company_name,
                    batch_index=i+1,
                    total_batches=len(batches),
                    max_output_tokens=max_output_tokens
                )
                extracted_summaries.append(batch_summary)
                print(f"   âœ… ç¬¬ {i+1} æ‰¹æ¬¡æå–å®Œæˆ")
            except Exception as e:
                print(f"   âŒ ç¬¬ {i+1} æ‰¹æ¬¡æå–å¤±è´¥: {e}")
                # å¤±è´¥æ—¶ä½¿ç”¨åŸå§‹æ•°æ®çš„ç®€è¦ä¿¡æ¯
                fallback_summary = self._create_fallback_summary(batch)
                extracted_summaries.append(fallback_summary)
        
        # åˆå¹¶æ‰€æœ‰æå–çš„ä¿¡æ¯
        final_summary = self._merge_extracted_summaries(
            section_title, extracted_summaries, len(relevant_data)
        )
        
        return {
            "section_title": section_title,
            "section_points": section_points,
            "collected_data": [{"summary": final_summary, "source_count": len(relevant_data)}],
            "processing_method": "batch_extraction",
            "total_tokens": self.token_calculator.count_tokens(final_summary),
            "original_data_count": len(relevant_data),
            "batch_count": len(batches),
            "company_name": company_name
        }
    
    def _create_data_batches(
        self, 
        data_items: List[Dict[str, Any]], 
        max_tokens_per_batch: int
    ) -> List[List[Dict[str, Any]]]:
        """
        å°†æ•°æ®åˆ†æ‰¹ï¼Œç¡®ä¿æ¯æ‰¹tokenæ•°ä¸è¶…è¿‡é™åˆ¶
        
        Args:
            data_items: æ•°æ®é¡¹åˆ—è¡¨
            max_tokens_per_batch: æ¯æ‰¹æœ€å¤§tokenæ•°
            
        Returns:
            åˆ†æ‰¹åçš„æ•°æ®åˆ—è¡¨
        """
        batches = []
        current_batch = []
        current_tokens = 0
        
        for item in data_items:
            content = item.get("summary", "") or item.get("content", "") or item.get("md", "")
            item_tokens = self.token_calculator.count_tokens(content)
            
            # å¦‚æœå•ä¸ªé¡¹ç›®å°±è¶…è¿‡é™åˆ¶ï¼Œå•ç‹¬æˆæ‰¹
            if item_tokens > max_tokens_per_batch:
                if current_batch:
                    batches.append(current_batch)
                    current_batch = []
                    current_tokens = 0
                batches.append([item])
                continue
            
            # æ£€æŸ¥æ·»åŠ å½“å‰é¡¹ç›®æ˜¯å¦ä¼šè¶…è¿‡é™åˆ¶
            if current_tokens + item_tokens > max_tokens_per_batch and current_batch:
                batches.append(current_batch)
                current_batch = [item]
                current_tokens = item_tokens
            else:
                current_batch.append(item)
                current_tokens += item_tokens
        
        # æ·»åŠ æœ€åä¸€æ‰¹
        if current_batch:
            batches.append(current_batch)
        
        return batches
    
    def _extract_batch_information(
        self,
        section_title: str,
        section_points: List[str],
        data_batch: List[Dict[str, Any]],
        company_name: str,
        batch_index: int,
        total_batches: int,
        max_output_tokens: int
    ) -> str:
        """
        ä»ä¸€æ‰¹æ•°æ®ä¸­æå–å…³é”®ä¿¡æ¯
        
        Args:
            section_title: ç« èŠ‚æ ‡é¢˜
            section_points: ç« èŠ‚è¦ç‚¹
            data_batch: æ•°æ®æ‰¹æ¬¡
            company_name: å…¬å¸åç§°
            batch_index: å½“å‰æ‰¹æ¬¡ç´¢å¼•
            total_batches: æ€»æ‰¹æ¬¡æ•°
            max_output_tokens: æœ€å¤§è¾“å‡ºtokenæ•°
            
        Returns:
            æå–çš„å…³é”®ä¿¡æ¯æ‘˜è¦
        """
        # æ„å»ºæ•°æ®å†…å®¹
        data_content = ""
        for item in data_batch:
            content = item.get("summary", "") or item.get("content", "") or item.get("md", "")
            data_content += f"\\n\\nã€æ•°æ®{item['id']}ã€‘{item.get('title', '')}\\n{content}"
        
        # æ„å»ºæå–æç¤º
        points_text = "\\n".join([f"- {point}" for point in section_points])
        
        system_prompt = f"""ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„é‡‘èæ•°æ®æ”¶é›†å’Œåˆ†æåŠ©æ‰‹ã€‚ä½ çš„ä»»åŠ¡æ˜¯ä»æä¾›çš„æ•°æ®ä¸­å…¨é¢æ”¶é›†ä¸ç‰¹å®šç« èŠ‚ç›¸å…³çš„æ‰€æœ‰æœ‰ä»·å€¼ä¿¡æ¯ï¼ŒåŒ…æ‹¬å…·ä½“æ•°æ®ã€ç»†èŠ‚å’Œæ·±åº¦åˆ†æå†…å®¹ã€‚

**æ”¶é›†ç›®æ ‡ç« èŠ‚**: {section_title}
**ç« èŠ‚è¦ç‚¹**:
{points_text}

**ä¿¡æ¯æ”¶é›†è¦æ±‚**:
1. **å…¨é¢æ€§æ”¶é›†**: ä¸ä»…è¦æå–å…³é”®ä¿¡æ¯æ‘˜è¦ï¼Œæ›´è¦æ”¶é›†å…·ä½“çš„æ•°æ®ã€æ•°å­—ã€æ¯”ä¾‹ã€è¶‹åŠ¿ç­‰ç»†èŠ‚ä¿¡æ¯
2. **æ•°æ®å®Œæ•´æ€§**: ä¿ç•™æ‰€æœ‰ç›¸å…³çš„è´¢åŠ¡æ•°æ®ã€ä¸šåŠ¡æŒ‡æ ‡ã€æ—¶é—´åºåˆ—æ•°æ®ã€å¯¹æ¯”æ•°æ®ç­‰
3. **ç»†èŠ‚ä¿ç•™**: åŒ…å«å…·ä½“çš„å…¬å¸åç§°ã€äº§å“åç§°ã€æŠ€æœ¯ç»†èŠ‚ã€åˆä½œä¼™ä¼´ã€ç›‘ç®¡è¦æ±‚ç­‰
4. **åˆ†æè§‚ç‚¹**: æ”¶é›†ä¸“å®¶è§‚ç‚¹ã€åˆ†æå¸ˆè¯„ä»·ã€å¸‚åœºé¢„æœŸã€é£é™©è¯„ä¼°ç­‰æ·±åº¦åˆ†æå†…å®¹
5. **æ—¶é—´ä¿¡æ¯**: ä¿ç•™å…·ä½“çš„æ—¶é—´èŠ‚ç‚¹ã€å‘å±•å†ç¨‹ã€é¢„æœŸæ—¶é—´è¡¨ç­‰æ—¶åºä¿¡æ¯
6. **å¼•ç”¨æ¥æº**: åœ¨æ”¶é›†ä¿¡æ¯æ—¶ä¿æŒæ•°æ®æ¥æºçš„æ ‡è¯†ï¼ˆå¦‚ã€æ•°æ®Xã€‘æ ¼å¼ï¼‰

**é‡ç‚¹æ”¶é›†å†…å®¹ç±»å‹**:
- å…·ä½“çš„è´¢åŠ¡æ•°å­—å’Œæ¯”ä¾‹æ•°æ®
- ä¸šåŠ¡è¿è¥çš„è¯¦ç»†æŒ‡æ ‡å’ŒKPI
- æŠ€æœ¯å‚æ•°ã€äº§å“è§„æ ¼ã€æ€§èƒ½æ•°æ®
- å¸‚åœºä»½é¢ã€ç«äº‰æ ¼å±€çš„å…·ä½“æ•°æ®
- ç›‘ç®¡æ”¿ç­–çš„å…·ä½“æ¡æ¬¾å’Œå½±å“
- åˆä½œåè®®çš„å…·ä½“å†…å®¹å’Œæ¡ä»¶
- é£é™©å› ç´ çš„å…·ä½“æè¿°å’Œé‡åŒ–æ•°æ®
- æœªæ¥è§„åˆ’çš„å…·ä½“ç›®æ ‡å’Œæ—¶é—´è¡¨

**è¾“å‡ºæ ¼å¼è¦æ±‚**:
1. æŒ‰ç…§ä¿¡æ¯ç±»å‹åˆ†æ®µç»„ç»‡ï¼Œä½†ä¿æŒæµç•…çš„æ®µè½å½¢å¼
2. æ¯ä¸ªé‡è¦æ•°æ®ç‚¹éƒ½è¦åŒ…å«å…·ä½“æ•°å€¼å’Œå•ä½
3. ä¿ç•™åŸæ–‡ä¸­çš„ä¸“ä¸šæœ¯è¯­å’ŒæŠ€æœ¯ç»†èŠ‚
4. å¯¹äºå¤æ‚çš„åˆ†æï¼Œä¿ç•™å®Œæ•´çš„é€»è¾‘é“¾æ¡
5. ç¡®ä¿ä¿¡æ¯çš„å®Œæ•´æ€§ï¼Œé¿å…è¿‡åº¦ç®€åŒ–

**è¾“å‡ºé•¿åº¦**: å°½å¯èƒ½è¯¦ç»†å’Œå®Œæ•´ï¼Œä¸è¦ä¸ºäº†ç®€æ´è€Œçœç•¥é‡è¦ç»†èŠ‚ã€‚ç›®æ ‡æ˜¯ä¸ºåç»­æŠ¥å‘Šå†™ä½œæä¾›å……åˆ†çš„ç´ ææ”¯æ’‘ã€‚"""

        user_prompt = f"""è¯·ä»ä»¥ä¸‹æ•°æ®ä¸­å…¨é¢æ”¶é›†ä¸"{section_title}"ç›¸å…³çš„æ‰€æœ‰æœ‰ä»·å€¼ä¿¡æ¯ï¼š

**æ”¶é›†è¦æ±‚**: 
- æå–æ‰€æœ‰ç›¸å…³çš„å…·ä½“æ•°æ®ã€æ•°å­—ã€ç»†èŠ‚ä¿¡æ¯
- ä¸è¦åªåšé«˜å±‚æ¬¡æ€»ç»“ï¼Œè¦ä¿ç•™å…·ä½“çš„ä¸šåŠ¡ç»†èŠ‚
- åŒ…å«æ‰€æœ‰ç›¸å…³çš„åˆ†æè§‚ç‚¹å’Œä¸“å®¶è¯„ä»·
- ä¿æŒä¿¡æ¯çš„å®Œæ•´æ€§å’Œå‡†ç¡®æ€§

**å…¬å¸**: {company_name}
**å¤„ç†è¿›åº¦**: ç¬¬{batch_index}/{total_batches}æ‰¹æ¬¡æ•°æ®

**æ•°æ®å†…å®¹**:
{data_content}

è¯·è¿›è¡Œå…¨é¢çš„ä¿¡æ¯æ”¶é›†ï¼ˆæ³¨æ„ï¼šè¦æ”¶é›†å…·ä½“ä¿¡æ¯å’Œæ•°æ®ç»†èŠ‚ï¼Œä¸æ˜¯ç®€å•æ‘˜è¦ï¼‰ï¼š"""

        try:
            response = chat_no_tool(
                user_content=user_prompt,
                system_content=system_prompt,
                api_key=self.api_key,
                base_url=self.base_url,
                model=self.model,
                temperature=0.3,
                max_tokens=max_output_tokens
            )
            return response.strip()
        except Exception as e:
            print(f"     âŒ æ‰¹æ¬¡æå–å¤±è´¥: {e}")
            raise e
    
    def _create_fallback_summary(self, data_batch: List[Dict[str, Any]]) -> str:
        """
        åˆ›å»ºå¤‡ç”¨æ‘˜è¦ï¼ˆå½“AIæå–å¤±è´¥æ—¶ä½¿ç”¨ï¼‰
        
        Args:
            data_batch: æ•°æ®æ‰¹æ¬¡
            
        Returns:
            å¤‡ç”¨æ‘˜è¦æ–‡æœ¬
        """
        summaries = []
        for item in data_batch:
            title = item.get("title", "")
            summary = item.get("summary", "") or item.get("content", "")[:200] + "..."
            summaries.append(f"{title}: {summary}")
        
        return "\\n\\n".join(summaries)
    
    def _merge_extracted_summaries(
        self, 
        section_title: str, 
        summaries: List[str], 
        original_count: int
    ) -> str:
        """
        åˆå¹¶å¤šä¸ªæ‰¹æ¬¡æå–çš„æ‘˜è¦
        
        Args:
            section_title: ç« èŠ‚æ ‡é¢˜
            summaries: æå–çš„æ‘˜è¦åˆ—è¡¨
            original_count: åŸå§‹æ•°æ®æ•°é‡
            
        Returns:
            åˆå¹¶åçš„æœ€ç»ˆæ‘˜è¦
        """
        if not summaries:
            return f"æœªèƒ½ä» {original_count} ä¸ªæ•°æ®æºä¸­æå–åˆ°ä¸ '{section_title}' ç›¸å…³çš„ä¿¡æ¯ã€‚"
        
        if len(summaries) == 1:
            return summaries[0]
        
        # å¤šä¸ªæ‘˜è¦éœ€è¦åˆå¹¶
        merged_content = f"åŸºäº {original_count} ä¸ªæ•°æ®æºçš„ç»¼åˆåˆ†æï¼š\\n\\n"
        for i, summary in enumerate(summaries, 1):
            merged_content += f"**æ•°æ®æ‰¹æ¬¡ {i}**: {summary}\\n\\n"
        
        return merged_content.strip()


def create_data_id_lookup_function(all_data: List[Dict[str, Any]]) -> callable:
    """
    åˆ›å»ºä¸€ä¸ªé€šè¿‡IDæŸ¥æ‰¾æ•°æ®çš„å‡½æ•°ï¼Œä¾›å¤§æ¨¡å‹ä½¿ç”¨
    
    Args:
        all_data: æ‰€æœ‰æ•°æ®çš„åˆ—è¡¨
        
    Returns:
        æŸ¥æ‰¾å‡½æ•°
    """
    id_to_data = {str(item["id"]): item for item in all_data}
    
    def get_data_by_id(data_id: str) -> Dict[str, Any]:
        """
        æ ¹æ®IDè·å–æ•°æ®é¡¹
        
        Args:
            data_id: æ•°æ®ID
            
        Returns:
            æ•°æ®é¡¹å­—å…¸ï¼Œå¦‚æœæ‰¾ä¸åˆ°åˆ™è¿”å›ç©ºå­—å…¸
        """
        return id_to_data.get(str(data_id), {})
    
    return get_data_by_id


def extract_data_references_from_text(text: str) -> List[str]:
    """
    ä»æ–‡æœ¬ä¸­æå–æ•°æ®å¼•ç”¨ID
    
    Args:
        text: åŒ…å«æ•°æ®å¼•ç”¨çš„æ–‡æœ¬
        
    Returns:
        æå–çš„æ•°æ®IDåˆ—è¡¨
    """
    import re
    
    # åŒ¹é…æ¨¡å¼ï¼šã€æ•°æ®123ã€‘æˆ–[æ•°æ®123]æˆ–(æ•°æ®123)
    patterns = [
        r'ã€æ•°æ®(\d+)ã€‘',
        r'\\[æ•°æ®(\d+)\\]',
        r'\\(æ•°æ®(\d+)\\)',
        r'æ•°æ®ID[ï¼š:](\d+)',
        r'å¼•ç”¨æ•°æ®(\d+)'
    ]
    
    data_ids = []
    for pattern in patterns:
        matches = re.findall(pattern, text)
        data_ids.extend(matches)
    
    return list(set(data_ids))  # å»é‡
