
import json
import asyncio
from typing import List, Dict, Any, Optional, Tuple
from financial_report.search_tools.search_tools import bing_search_with_cache
from financial_report.llm_calls.company_outline_search_queries import generate_search_queries
from financial_report.llm_calls.content_assessor import assess_content_quality_hybrid
from data_process.content_summarizer import generate_summaries_for_collected_data
from data_process.outline_data_allocator import allocate_data_to_outline_sync


# æœ€å°å†…å®¹é•¿åº¦å¸¸é‡
MIN_CONTENT_LENGTH = 50


class SearchDataProcessor:
    """æœç´¢æ•°æ®å¤„ç†å™¨ç±» - å°è£…æ‰€æœ‰ç›¸å…³åŠŸèƒ½"""
    
    def __init__(self, 
                 api_key: str,
                 base_url: str,
                 model: str,
                 summary_api_key: str,
                 summary_base_url: str,
                 summary_model: str):
        """
        åˆå§‹åŒ–æœç´¢æ•°æ®å¤„ç†å™¨
        
        Args:
            api_key: è´¨é‡è¯„ä¼°APIå¯†é’¥
            base_url: è´¨é‡è¯„ä¼°APIåŸºç¡€URL
            model: è´¨é‡è¯„ä¼°æ¨¡å‹åç§°
            summary_api_key: æ‘˜è¦ç”ŸæˆAPIå¯†é’¥
            summary_base_url: æ‘˜è¦ç”ŸæˆAPIåŸºç¡€URL
            summary_model: æ‘˜è¦ç”Ÿæˆæ¨¡å‹åç§°
        """
        self.api_key = api_key
        self.base_url = base_url
        self.model = model
        self.summary_api_key = summary_api_key
        self.summary_base_url = summary_base_url
        self.summary_model = summary_model
    
    @staticmethod
    def format_search_results_to_flattened_data(
        search_results: List[Dict[str, Any]], 
        company_name: str = "",
        search_query: str = "",
        start_id: int = 1
    ) -> List[Dict[str, Any]]:
        """
        å°†æœç´¢ç»“æœæ ¼å¼åŒ–ä¸ºä¸å±•å¹³æ•°æ®ä¸€è‡´çš„æ ¼å¼
        
        Args:
            search_results: bing_search_with_cacheè¿”å›çš„æœç´¢ç»“æœ
            company_name: å…¬å¸åç§°
            search_query: æœç´¢æŸ¥è¯¢
            start_id: èµ·å§‹IDï¼ˆç”¨äºé¿å…ä¸ç°æœ‰æ•°æ®IDå†²çªï¼‰
            
        Returns:
            æ ¼å¼åŒ–åçš„æ•°æ®åˆ—è¡¨ï¼Œä¸flattened_tonghuashun_dataæ ¼å¼ä¸€è‡´
        """
        if not search_results:
            return []
        
        flattened_results = []
        
        for i, result in enumerate(search_results):
            # åŸºæœ¬æ ¼å¼åŒ–ï¼Œä¸å±•å¹³æ•°æ®ä¿æŒä¸€è‡´
            flattened_record = {
                "id": str(start_id + i),
                "company_name": company_name,
                "company_code": "",
                "market": "",
                "tonghuashun_total_code": "",
                "url": result.get("url", ""),
                "title": result.get("title", ""),
                "data_source_type": result.get("data_source_type", "html"),
                "content": result.get("md", ""),
                "search_query": search_query,
                "data_source": "search_result"
            }
            flattened_results.append(flattened_record)
        
        return flattened_results
    
    def _log_progress(self, current: int, total: int, message: str, result: str = ""):
        """ç»Ÿä¸€çš„è¿›åº¦æ—¥å¿—è¾“å‡º"""
        prefix = f"   ğŸ”„ [{current}/{total}]" if result == "" else f"   {result} [{current}/{total}]"
        print(f"{prefix} {message}")
    
    def assess_search_results_quality(
        self,
        search_results: List[Dict[str, Any]],
        company_name: str,
        section_title: str,
        max_concurrent: int = 5
    ) -> List[Dict[str, Any]]:
        """
        å¯¹æœç´¢ç»“æœè¿›è¡Œè´¨é‡è¯„ä¼°ï¼Œè¿‡æ»¤ä½è´¨é‡å†…å®¹
        
        Args:
            search_results: æœç´¢ç»“æœåˆ—è¡¨
            company_name: å…¬å¸åç§°
            section_title: ç« èŠ‚æ ‡é¢˜ï¼ˆç”¨äºæ„å»ºæŸ¥è¯¢ç›®æ ‡ï¼‰
            max_concurrent: æœ€å¤§å¹¶å‘æ•°
            
        Returns:
            è´¨é‡è¯„ä¼°åçš„æœç´¢ç»“æœåˆ—è¡¨ï¼ˆåªåŒ…å«é«˜è´¨é‡å†…å®¹ï¼‰
        """
        if not search_results:
            return []
        
        print(f"ğŸ” å¼€å§‹å¯¹ {len(search_results)} ä¸ªæœç´¢ç»“æœè¿›è¡Œè´¨é‡è¯„ä¼°ï¼ˆå¹¶å‘æ•°: {max_concurrent}ï¼‰...")
        
        # æ„å»ºæŸ¥è¯¢ç›®æ ‡
        query_objective = f"{company_name} {section_title}"
        
        # ä½¿ç”¨å¼‚æ­¥å¹¶å‘å¤„ç†
        return asyncio.run(self._assess_quality_concurrent(
            search_results, query_objective, max_concurrent
        ))
    
    async def _assess_quality_concurrent(
        self,
        search_results: List[Dict[str, Any]],
        query_objective: str,
        max_concurrent: int
    ) -> List[Dict[str, Any]]:
        """å¼‚æ­¥å¹¶å‘è´¨é‡è¯„ä¼°"""
        semaphore = asyncio.Semaphore(max_concurrent)
        
        async def assess_single_result(i: int, result: Dict) -> Optional[Dict]:
            async with semaphore:
                try:
                    url = result.get("url", "")
                    content = result.get("md", "")
                    title = result.get("title", "")
                    
                    # é¢„è¿‡æ»¤ï¼šæ£€æŸ¥å†…å®¹é•¿åº¦
                    if not content or len(content.strip()) < MIN_CONTENT_LENGTH:
                        self._log_progress(i+1, len(search_results), f"è·³è¿‡ç©ºå†…å®¹: {title[:50]}...", "âš ï¸")
                        return None
                    
                    self._log_progress(i+1, len(search_results), f"è¯„ä¼°: {title[:50]}...")
                    
                    # åœ¨å¼‚æ­¥ä¸Šä¸‹æ–‡ä¸­è¿è¡ŒåŒæ­¥çš„è´¨é‡è¯„ä¼°
                    loop = asyncio.get_event_loop()
                    assessment = await loop.run_in_executor(
                        None,
                        lambda: assess_content_quality_hybrid(
                            cleaned_text=content,
                            url=url,
                            query_objective=query_objective,
                            chat_model=self.model,
                            api_key=self.api_key,
                            base_url=self.base_url
                        )
                    )
                    
                    is_high_quality = assessment.get("is_high_quality", False)
                    reason = assessment.get("reason", "æœªçŸ¥åŸå› ")
                    source = assessment.get("source", "unknown")
                    
                    if is_high_quality:
                        # æ·»åŠ è´¨é‡è¯„ä¼°ä¿¡æ¯
                        result["quality_assessment"] = {
                            "is_high_quality": True,
                            "reason": reason,
                            "source": source,
                            "assessed_for": query_objective
                        }
                        self._log_progress(i+1, len(search_results), f"é«˜è´¨é‡å†…å®¹ ({source}): {reason[:50]}...", "âœ…")
                        return result
                    else:
                        self._log_progress(i+1, len(search_results), f"ä½è´¨é‡å†…å®¹ ({source}): {reason[:50]}...", "âŒ")
                        return None
                        
                except Exception as e:
                    self._log_progress(i+1, len(search_results), f"è¯„ä¼°å¤±è´¥: {e}", "âš ï¸")
                    return None
        
        # å¹¶å‘æ‰§è¡Œæ‰€æœ‰è¯„ä¼°ä»»åŠ¡
        tasks = [assess_single_result(i, result) for i, result in enumerate(search_results)]
        results = await asyncio.gather(*tasks, return_exceptions=True)
        
        # è¿‡æ»¤å‡ºé«˜è´¨é‡ç»“æœå’Œç»Ÿè®¡ä¿¡æ¯
        high_quality_results = []
        stats = {"low_quality": 0, "error": 0}
        
        for result in results:
            if isinstance(result, Exception):
                stats["error"] += 1
            elif result is None:
                stats["low_quality"] += 1
            else:
                high_quality_results.append(result)
        
        # è¾“å‡ºç»Ÿè®¡ä¿¡æ¯
        self._print_quality_stats(high_quality_results, stats, len(search_results))
        return high_quality_results
    
    def _print_quality_stats(self, high_quality_results: List, stats: Dict, total: int):
        """è¾“å‡ºè´¨é‡è¯„ä¼°ç»Ÿè®¡ä¿¡æ¯"""
        print(f"ğŸ“Š è´¨é‡è¯„ä¼°å®Œæˆ:")
        print(f"   âœ… é«˜è´¨é‡å†…å®¹: {len(high_quality_results)}")
        print(f"   âŒ ä½è´¨é‡å†…å®¹: {stats['low_quality']}")
        print(f"   âš ï¸  è¯„ä¼°å¤±è´¥: {stats['error']}")
        print(f"   ğŸ“ˆ è´¨é‡é€šè¿‡ç‡: {len(high_quality_results)/total*100:.1f}%")
    
    def process_search_results_with_summary(
        self,
        search_results: List[Dict[str, Any]],
        company_name: str,
        search_query: str,
        section_title: str,
        start_id: int,
        chat_max_token_length: int = 128 * 1024,
        max_summary_length: int = 800,
        max_concurrent_summary: int = 10,
        max_concurrent_assessment: int = 5,
        enable_quality_assessment: bool = True
    ) -> List[Dict[str, Any]]:
        """
        å°†æœç´¢ç»“æœæ ¼å¼åŒ–ã€è´¨é‡è¯„ä¼°å¹¶ç”Ÿæˆæ‘˜è¦
        ä¸€ç«™å¼å¤„ç†å‡½æ•°ï¼Œæ›¿ä»£åŸæ¥çš„generate_search_results_with_summary
        """
        print(f"ğŸ”„ å¼€å§‹å¤„ç†æœç´¢ç»“æœ...")
        
        if not search_results:
            print("âš ï¸  æœç´¢ç»“æœä¸ºç©º")
            return []
        
        # 1. è´¨é‡è¯„ä¼°ï¼ˆå¯é€‰ï¼‰
        if enable_quality_assessment:
            print(f"ğŸ” å¯ç”¨è´¨é‡è¯„ä¼°...")
            processed_results = self.assess_search_results_quality(
                search_results=search_results,
                company_name=company_name,
                section_title=section_title,
                max_concurrent=max_concurrent_assessment
            )
            
            if not processed_results:
                print("âŒ æ²¡æœ‰é€šè¿‡è´¨é‡è¯„ä¼°çš„å†…å®¹")
                return []
                
            print(f"âœ… è´¨é‡è¯„ä¼°å®Œæˆï¼Œä¿ç•™ {len(processed_results)}/{len(search_results)} ä¸ªé«˜è´¨é‡ç»“æœ")
        else:
            print(f"âš ï¸  è·³è¿‡è´¨é‡è¯„ä¼°")
            processed_results = search_results
        
        # 2. æ ¼å¼åŒ–æœç´¢ç»“æœ
        print(f"ğŸ“‹ æ ¼å¼åŒ– {len(processed_results)} ä¸ªæœç´¢ç»“æœ...")
        
        flattened_results = self.format_search_results_to_flattened_data(
            search_results=processed_results,
            company_name=company_name,
            search_query=search_query,
            start_id=start_id
        )
        
        print(f"ğŸ“‹ å·²æ ¼å¼åŒ– {len(flattened_results)} æ¡æœç´¢ç»“æœ")
        
        # 3. ç”Ÿæˆæ‘˜è¦
        if flattened_results:
            print(f"ğŸ”„ å¼€å§‹ä¸ºæœç´¢ç»“æœç”Ÿæˆæ‘˜è¦...")
            try:
                flattened_results = generate_summaries_for_collected_data(
                    data_items=flattened_results,
                    api_key=self.summary_api_key,
                    base_url=self.summary_base_url,
                    model=self.summary_model,
                    chat_max_token_length=chat_max_token_length,
                    max_summary_length=max_summary_length,
                    max_concurrent=max_concurrent_summary
                )
                print(f"âœ… æœç´¢ç»“æœæ‘˜è¦ç”Ÿæˆå®Œæˆï¼")
            except Exception as e:
                print(f"âŒ æœç´¢ç»“æœæ‘˜è¦ç”Ÿæˆå¤±è´¥: {e}")
                print("ğŸ“‹ å°†è¿”å›ä¸å«æ‘˜è¦çš„æœç´¢ç»“æœ...")
        
        return flattened_results
    
    def _execute_single_search(
        self, 
        query: str, 
        search_api_url: str, 
        max_results: int
    ) -> Tuple[bool, List[Dict[str, Any]]]:
        """æ‰§è¡Œå•æ¬¡æœç´¢"""
        try:
            search_results = bing_search_with_cache(
                query=query,
                search_api_url=search_api_url,
                total=max_results,
                force_refresh=False
            )
            
            if not search_results:
                print(f"   âš ï¸  æœç´¢æ— ç»“æœ")
                return False, []
                
            print(f"   ğŸ“Š æœç´¢åˆ° {len(search_results)} ä¸ªç»“æœ")
            return True, search_results
            
        except Exception as e:
            print(f"   âŒ æœç´¢å¤±è´¥: {e}")
            return False, []
    
    def _test_data_matching(
        self, 
        section: Dict[str, Any], 
        formatted_results: List[Dict[str, Any]],
        max_concurrent: int = 10
    ) -> Tuple[bool, List[str]]:
        """æµ‹è¯•æ•°æ®æ˜¯å¦èƒ½åŒ¹é…åˆ°ç« èŠ‚"""
        print(f"   ğŸ”— æµ‹è¯•æ•°æ®åŒ¹é…...")
        
        # åˆ›å»ºä¸´æ—¶å¤§çº²æ•°æ®è¿›è¡ŒåŒ¹é…æµ‹è¯•
        temp_outline = {"reportOutline": [section]}
        
        try:
            temp_allocation = allocate_data_to_outline_sync(
                outline_data=temp_outline,
                flattened_data=formatted_results,
                api_key=self.summary_api_key,
                base_url=self.summary_base_url,
                model=self.summary_model,
                max_concurrent=max_concurrent
            )
            
            # æ£€æŸ¥æ˜¯å¦æœ‰æ•°æ®åŒ¹é…åˆ°è¿™ä¸ªç« èŠ‚
            allocated_sections = temp_allocation.get("outline_with_allocations", {}).get("reportOutline", [])
            if allocated_sections and allocated_sections[0].get("allocated_data_ids"):
                matched_ids = allocated_sections[0]["allocated_data_ids"]
                print(f"   âœ… åŒ¹é…æˆåŠŸï¼åŒ¹é…äº† {len(matched_ids)} ä¸ªæ•°æ®é¡¹")
                return True, matched_ids
            else:
                print(f"   âŒ æœ¬è½®æœç´¢æœªåŒ¹é…ï¼Œç»§ç»­ä¸‹ä¸€ä¸ªæŸ¥è¯¢...")
                return False, []
                
        except Exception as e:
            print(f"   âŒ åŒ¹é…æµ‹è¯•å¤±è´¥: {e}")
            return False, []
    
    def smart_search_for_empty_sections(
        self,
        empty_sections: List[Dict[str, Any]],
        company_name: str,
        existing_flattened_data: List[Dict[str, Any]],
        search_api_url: str,
        chat_max_token_length: int = 128 * 1024,
        max_searches_per_section: int = 3,
        max_results_per_search: int = 10,
        max_concurrent_summary: int = 10
    ) -> Dict[str, Any]:
        """
        ä¸ºæ²¡æœ‰åŒ¹é…æ•°æ®çš„ç« èŠ‚æ™ºèƒ½æœç´¢ç›¸å…³å†…å®¹
        """
        print(f"\nğŸ” å¼€å§‹ä¸º {len(empty_sections)} ä¸ªæ— æ•°æ®ç« èŠ‚æ™ºèƒ½æœç´¢...")
        
        all_new_data = []
        section_search_results = {}
        current_max_id = max([int(item["id"]) for item in existing_flattened_data], default=0)
        
        for section_idx, section in enumerate(empty_sections):
            result = self._process_single_section(
                section, section_idx, len(empty_sections), company_name, 
                search_api_url, current_max_id, max_searches_per_section,
                max_results_per_search, chat_max_token_length, max_concurrent_summary
            )
            
            section_search_results[section["title"]] = result["summary"]
            if result["matched"]:
                all_new_data.extend(result["data"])
                current_max_id = result["new_max_id"]
        
        # æ±‡æ€»ç»“æœ
        return self._generate_final_search_summary(empty_sections, section_search_results, all_new_data)
    
    def _process_single_section(
        self, section: Dict[str, Any], section_idx: int, total_sections: int,
        company_name: str, search_api_url: str, current_max_id: int,
        max_searches_per_section: int, max_results_per_search: int,
        chat_max_token_length: int, max_concurrent_summary: int
    ) -> Dict[str, Any]:
        """å¤„ç†å•ä¸ªç« èŠ‚çš„æœç´¢"""
        section_title = section["title"]
        section_points = section.get("points", [])
        
        print(f"\nğŸ“‹ å¤„ç†ç« èŠ‚ {section_idx + 1}/{total_sections}: {section_title}")
        print(f"   ğŸ“ ç« èŠ‚è¦ç‚¹æ•°é‡: {len(section_points)}")
        
        # 1. ç”Ÿæˆæœç´¢æŸ¥è¯¢
        try:
            search_queries = generate_search_queries(
                company=company_name,
                section_title=section_title,
                section_points=section_points,
                api_key=self.api_key,
                base_url=self.base_url,
                model=self.model,
                max_queries=max_searches_per_section
            )
            print(f"   âœ… ç”Ÿæˆäº† {len(search_queries)} ä¸ªæœç´¢æŸ¥è¯¢")
        except Exception as e:
            print(f"   âŒ æœç´¢æŸ¥è¯¢ç”Ÿæˆå¤±è´¥: {e}")
            return {"matched": False, "data": [], "new_max_id": current_max_id, 
                   "summary": {"matched": False, "queries_used": 0, "data_found": 0}}
        
        # 2. æ‰§è¡Œæœç´¢å¹¶æ£€æŸ¥åŒ¹é…
        section_matched = False
        section_new_data = []
        
        for query_idx, query in enumerate(search_queries):
            if section_matched:
                print(f"   â­ï¸  ç« èŠ‚å·²åŒ¹é…ï¼Œè·³è¿‡å‰©ä½™æŸ¥è¯¢")
                break
                
            print(f"   ğŸŒ æ‰§è¡Œæœç´¢ {query_idx + 1}/{len(search_queries)}: {query[:50]}...")
            
            # æ‰§è¡Œæœç´¢
            success, search_results = self._execute_single_search(query, search_api_url, max_results_per_search)
            if not success:
                continue
            
            # æ ¼å¼åŒ–æœç´¢ç»“æœå¹¶ç”Ÿæˆæ‘˜è¦
            # ä½¿ç”¨å½“å‰æœ€å¤§ID+1ä½œä¸ºèµ·å§‹IDï¼Œç¡®ä¿ä¸å†²çª
            next_start_id = current_max_id + 1
            formatted_results = self.process_search_results_with_summary(
                search_results=search_results,
                company_name=company_name,
                search_query=query,
                section_title=section_title,
                start_id=next_start_id,
                chat_max_token_length=chat_max_token_length,
                max_concurrent_summary=max_concurrent_summary,
                max_concurrent_assessment=5,
                enable_quality_assessment=True
            )
            
            if not formatted_results:
                print(f"   âš ï¸  æ ¼å¼åŒ–ç»“æœä¸ºç©º")
                continue
            
            # æ›´æ–°IDè®¡æ•°å™¨åˆ°æœ€æ–°çš„æœ€å¤§å€¼
            if formatted_results:
                max_result_id = max([int(item["id"]) for item in formatted_results])
                current_max_id = max_result_id
                print(f"   ğŸ”¢ æ›´æ–°IDè®¡æ•°å™¨: {current_max_id}")
            
            # æµ‹è¯•æ•°æ®åŒ¹é…
            matched, matched_ids = self._test_data_matching(section, formatted_results, max_concurrent_summary)
            if matched:
                section_matched = True
                section_new_data.extend(formatted_results)
                break
        
        result_summary = {
            "matched": section_matched,
            "queries_used": len(search_queries),
            "data_found": len(section_new_data)
        }
        
        if section_matched:
            print(f"   ğŸ‰ ç« èŠ‚ '{section_title}' æœç´¢æˆåŠŸï¼Œæ‰¾åˆ° {len(section_new_data)} ä¸ªç›¸å…³æ•°æ®")
        else:
            print(f"   ğŸ˜” ç« èŠ‚ '{section_title}' æœªæ‰¾åˆ°åŒ¹é…æ•°æ®")
        
        return {
            "matched": section_matched,
            "data": section_new_data,
            "new_max_id": current_max_id,
            "summary": result_summary
        }
    
    def _generate_final_search_summary(
        self, empty_sections: List, section_search_results: Dict, all_new_data: List
    ) -> Dict[str, Any]:
        """ç”Ÿæˆæœ€ç»ˆæœç´¢æ±‡æ€»"""
        total_matched = sum(1 for result in section_search_results.values() if result["matched"])
        total_data_found = len(all_new_data)
        
        print(f"\nğŸ“Š æ™ºèƒ½æœç´¢å®Œæˆæ±‡æ€»:")
        print(f"   ğŸ“‹ å¤„ç†ç« èŠ‚: {len(empty_sections)}")
        print(f"   âœ… æˆåŠŸåŒ¹é…: {total_matched}")
        print(f"   ğŸ“„ æ–°å¢æ•°æ®: {total_data_found}")
        print(f"   ğŸ’” ä»æ— æ•°æ®: {len(empty_sections) - total_matched}")
        
        return {
            "new_search_data": all_new_data,
            "search_results_summary": section_search_results,
            "stats": {
                "total_sections_processed": len(empty_sections),
                "sections_matched": total_matched,
                "sections_still_empty": len(empty_sections) - total_matched,
                "total_new_data": total_data_found
            }
        }
    
    @staticmethod
    def merge_search_data_with_existing(
        existing_flattened_data: List[Dict[str, Any]],
        new_search_data: List[Dict[str, Any]]
    ) -> List[Dict[str, Any]]:
        """
        å°†æ–°æœç´¢çš„æ•°æ®ä¸ç°æœ‰æ•°æ®åˆå¹¶ï¼Œç¡®ä¿IDä¸å†²çª
        """
        print(f"ğŸ”— åˆå¹¶æ•°æ®ï¼šç°æœ‰ {len(existing_flattened_data)} æ¡ + æ–°å¢ {len(new_search_data)} æ¡")
        
        if not new_search_data:
            print(f"âš ï¸  æ²¡æœ‰æ–°æ•°æ®éœ€è¦åˆå¹¶")
            return existing_flattened_data.copy()
        
        # 1. æ‰¾åˆ°ç°æœ‰æ•°æ®ä¸­çš„æœ€å¤§ID
        existing_ids = [int(item["id"]) for item in existing_flattened_data if item.get("id")]
        max_existing_id = max(existing_ids) if existing_ids else 0
        
        print(f"ğŸ”¢ ç°æœ‰æ•°æ®æœ€å¤§ID: {max_existing_id}")
        
        # 2. ä¸ºæ–°æ•°æ®é‡æ–°åˆ†é…IDï¼Œç¡®ä¿ä¸ä¸ç°æœ‰IDå†²çª
        reassigned_new_data = []
        new_id_start = max_existing_id + 1
        
        for i, item in enumerate(new_search_data):
            new_item = item.copy()  # åˆ›å»ºå‰¯æœ¬é¿å…ä¿®æ”¹åŸæ•°æ®
            old_id = new_item.get("id", "unknown")
            new_id = str(new_id_start + i)
            new_item["id"] = new_id
            reassigned_new_data.append(new_item)
            
            # è®°å½•IDæ˜ å°„ç”¨äºè°ƒè¯•
            if i < 5:  # åªæ˜¾ç¤ºå‰5ä¸ªçš„æ˜ å°„
                print(f"   ğŸ”„ IDé‡æ–°åˆ†é…: {old_id} â†’ {new_id}")
        
        if len(new_search_data) > 5:
            print(f"   ... è¿˜æœ‰ {len(new_search_data) - 5} ä¸ªæ•°æ®çš„IDè¢«é‡æ–°åˆ†é…")
        
        # 3. åˆå¹¶æ•°æ®
        merged_data = existing_flattened_data.copy()
        merged_data.extend(reassigned_new_data)
        
        # 4. éªŒè¯IDå”¯ä¸€æ€§
        all_ids = [item["id"] for item in merged_data]
        unique_ids = set(all_ids)
        
        if len(all_ids) != len(unique_ids):
            duplicate_ids = [id for id in all_ids if all_ids.count(id) > 1]
            print(f"âš ï¸  è­¦å‘Šï¼šå‘ç°é‡å¤ID: {set(duplicate_ids)}")
        else:
            print(f"âœ… IDå”¯ä¸€æ€§éªŒè¯é€šè¿‡")
        
        print(f"âœ… åˆå¹¶å®Œæˆï¼Œæ€»è®¡ {len(merged_data)} æ¡æ•°æ®")
        print(f"ğŸ“Š IDèŒƒå›´: 1 - {max([int(item['id']) for item in merged_data])}")
        
        return merged_data







