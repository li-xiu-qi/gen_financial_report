"""
å¤§çº²æ•°æ®æ™ºèƒ½åˆ†é…å™¨
ä¸ºæ¯ä¸ªæ•°æ®chunkæ™ºèƒ½åŒ¹é…æœ€åˆé€‚çš„å¤§çº²ç« èŠ‚ï¼Œä¼˜å…ˆç›¸å…³åº¦æœ€é«˜çš„åŒ¹é…
"""
import json
import asyncio
from typing import List, Dict, Any
from financial_report.utils.async_chat import async_chat_no_tool


# å•ä¸ªchunkåŒ¹é…æç¤ºè¯
CHUNK_MATCHING_PROMPT = """
ä½ æ˜¯ä¸€åä¸“ä¸šçš„é‡‘èåˆ†æå¸ˆï¼Œæ“…é•¿ä¸ºç ”ç©¶æŠ¥å‘Šæ•°æ®åŒ¹é…æœ€åˆé€‚çš„ç« èŠ‚ã€‚

## ä»»åŠ¡è¯´æ˜ï¼š
åˆ†ææä¾›çš„æ•°æ®å†…å®¹ï¼Œä»å¤§çº²ç« èŠ‚åˆ—è¡¨ä¸­æ‰¾å‡º**æœ€ç›¸å…³**çš„ç« èŠ‚è¿›è¡ŒåŒ¹é…ã€‚

## åŒ¹é…åŸåˆ™ï¼š
1. **ä¼˜å…ˆåŒ¹é…**ï¼šå³ä½¿ç›¸å…³åº¦ä¸æ˜¯100%ï¼Œä¹Ÿè¦æ‰¾å‡ºæœ€ç›¸å…³çš„ç« èŠ‚
2. **æœ€ä½³åŒ¹é…**ï¼šä»æ‰€æœ‰ç« èŠ‚ä¸­é€‰æ‹©ç›¸å…³åº¦æœ€é«˜çš„1ä¸ªç« èŠ‚
3. **æå°‘ä¸åŒ¹é…**ï¼šåªæœ‰å½“å†…å®¹ä¸æ‰€æœ‰ç« èŠ‚éƒ½å®Œå…¨æ— å…³æ—¶æ‰é€‰æ‹©ä¸åŒ¹é…

## è¾“å‡ºæ ¼å¼ï¼š
è¯·ä¸¥æ ¼æŒ‰ç…§ä»¥ä¸‹JSONæ ¼å¼è¾“å‡ºï¼š

**åŒ¹é…ç« èŠ‚æ—¶ï¼š**
```json
{{
  "section_index": 0,
  "section_title": "åŒ¹é…çš„ç« èŠ‚æ ‡é¢˜",
  "confidence_score": 0.85,
  "match_reason": "åŒ¹é…ç†ç”±è¯´æ˜",
  "content_summary": "æ•°æ®å†…å®¹ç®€è¦æè¿°"
}}
```

**å®Œå…¨ä¸åŒ¹é…æ—¶ï¼ˆæå°‘æƒ…å†µï¼‰ï¼š**
```json
{{
  "section_index": -1,
  "section_title": "",
  "confidence_score": 0.0,
  "match_reason": "ä¸æ‰€æœ‰ç« èŠ‚éƒ½æ— å…³çš„å…·ä½“åŸå› ",
  "content_summary": "æ•°æ®å†…å®¹ç®€è¦æè¿°"
}}
```

---

**ç ”ç©¶æŠ¥å‘Šå¤§çº²ç« èŠ‚åˆ—è¡¨ï¼š**
{outline_sections}

**æ•°æ®å†…å®¹ (ID: {data_id})ï¼š**
æ ‡é¢˜: {data_title}
å†…å®¹æ‘˜è¦: {data_summary}

è¯·åˆ†æå¹¶è¿”å›æœ€ä½³åŒ¹é…ç»“æœï¼š
"""


async def allocate_data_to_outline_reverse(
    outline_data: Dict[str, Any],
    flattened_data: List[Dict[str, Any]],
    api_key: str,
    base_url: str,
    model: str,
    max_output_tokens: int = 8 * 1024,
    max_concurrent: int = 10
) -> Dict[str, Any]:
    """
    æ™ºèƒ½åŒ¹é…ï¼šä¸ºæ¯ä¸ªæ•°æ®chunkæ‰¾åˆ°æœ€åˆé€‚çš„å¤§çº²ç« èŠ‚
    
    Args:
        outline_data: å¤§çº²æ•°æ®
        flattened_data: å±•å¹³åçš„æ•°æ®åˆ—è¡¨
        api_key: APIå¯†é’¥
        base_url: APIåŸºç¡€URL
        model: ä½¿ç”¨çš„æ¨¡å‹åç§°
        max_output_tokens: æœ€å¤§è¾“å‡ºtokens
        max_concurrent: æœ€å¤§å¹¶å‘æ•°
    
    Returns:
        åŒ…å«åˆ†é…ç»“æœçš„å®Œæ•´å¤§çº²
    """
    print(f"\nğŸ“‹ å¼€å§‹æ™ºèƒ½åŒ¹é… {len(flattened_data)} ä¸ªæ•°æ®é¡¹...")
    
    # å‡†å¤‡å¤§çº²ç« èŠ‚åˆ—è¡¨
    outline_sections = _format_outline_sections(outline_data)
    
    print(f"ğŸ“Š å¤„ç†ç»Ÿè®¡:")
    print(f"   - å¤§çº²ç« èŠ‚æ•°: {len(outline_data.get('reportOutline', []))}")
    print(f"   - æ•°æ®é¡¹æ•°é‡: {len(flattened_data)}")
    print(f"   - æœ€å¤§å¹¶å‘æ•°: {max_concurrent}")
    
    # åˆ›å»ºå¹¶å‘ä»»åŠ¡
    semaphore = asyncio.Semaphore(max_concurrent)
    tasks = []
    
    for i, data_item in enumerate(flattened_data):
        task = _match_single_chunk(
            data_item=data_item,
            outline_sections=outline_sections,
            api_key=api_key,
            base_url=base_url,
            model=model,
            max_output_tokens=max_output_tokens,
            semaphore=semaphore,
            item_index=i + 1,
            total_items=len(flattened_data)
        )
        tasks.append(task)
    
    print(f"ğŸš€ å¼€å§‹å¹¶å‘å¤„ç† {len(tasks)} ä¸ªåŒ¹é…ä»»åŠ¡...")
    
    # å¹¶å‘æ‰§è¡Œæ‰€æœ‰ä»»åŠ¡
    try:
        results = await asyncio.gather(*tasks, return_exceptions=True)
        print(f"âœ… æ‰€æœ‰ä»»åŠ¡æ‰§è¡Œå®Œæˆ")
    except Exception as e:
        print(f"âŒ å¹¶å‘æ‰§è¡Œå‡ºç°å¼‚å¸¸: {e}")
        results = []
    
    # å¤„ç†ç»“æœå¹¶æ„å»ºæœ€ç»ˆå¤§çº²
    final_outline = _build_final_outline(outline_data, results)
    
    # ç”Ÿæˆç»Ÿè®¡ä¿¡æ¯
    stats = _generate_matching_stats(results, len(flattened_data))
    
    print(f"\nğŸ“ˆ åŒ¹é…ç»Ÿè®¡:")
    print(f"   - æˆåŠŸåŒ¹é…: {stats['matched_count']}")
    print(f"   - ä¸åŒ¹é…: {stats['unmatched_count']}")
    print(f"   - å¤„ç†å¤±è´¥: {stats['failed_count']}")
    print(f"   - åŒ¹é…ç‡: {stats['match_rate']:.1f}%")
    print(f"   - å¹³å‡ç½®ä¿¡åº¦: {stats['avg_confidence']:.2f}")
    
    return {
        "outline_with_allocations": final_outline,
        "allocation_stats": stats
    }


def _format_outline_sections(outline_data: Dict[str, Any]) -> str:
    """æ ¼å¼åŒ–å¤§çº²ç« èŠ‚åˆ—è¡¨ä¸ºå­—ç¬¦ä¸²"""
    lines = []
    
    for i, section in enumerate(outline_data.get('reportOutline', [])):
        lines.append(f"{i}. {section.get('title', '')}")
        # æ·»åŠ è¦ç‚¹ä¿¡æ¯å¸®åŠ©åŒ¹é…
        points = section.get('points', [])
        if points:
            for point in points[:3]:  # åªæ˜¾ç¤ºå‰3ä¸ªè¦ç‚¹
                lines.append(f"   - {point}")
    
    return "\n".join(lines)


async def _match_single_chunk(
    data_item: Dict[str, Any],
    outline_sections: str,
    api_key: str,
    base_url: str,
    model: str,
    max_output_tokens: int,
    semaphore: asyncio.Semaphore,
    item_index: int,
    total_items: int
) -> Dict[str, Any]:
    """åŒ¹é…å•ä¸ªæ•°æ®chunkåˆ°æœ€åˆé€‚çš„ç« èŠ‚"""
    
    async with semaphore:
        data_id = data_item.get('id', '')
        data_title = data_item.get('title', '')
        data_summary = data_item.get('summary', '')
        
        # å¦‚æœæ²¡æœ‰æ‘˜è¦ï¼Œä½¿ç”¨å†…å®¹çš„å‰300å­—ç¬¦
        if not data_summary:
            content = data_item.get('content', '')
            data_summary = content[:300] + "..." if len(content) > 300 else content
        
        # æ„å»ºæç¤ºè¯
        prompt = CHUNK_MATCHING_PROMPT.format(
            outline_sections=outline_sections,
            data_id=data_id,
            data_title=data_title,
            data_summary=data_summary
        )
        
        print(f"   ğŸ”„ [{item_index}/{total_items}] å¤„ç†æ•°æ®é¡¹ ID:{data_id}")
        
        try:
            # è°ƒç”¨AI
            result = await async_chat_no_tool(
                user_content=prompt,
                api_key=api_key,
                base_url=base_url,
                model=model,
                max_tokens=max_output_tokens,
                temperature=0.2,  # é™ä½æ¸©åº¦ï¼Œæé«˜ä¸€è‡´æ€§
                use_cache=False,
                timeout=60
            )
            
            # è§£æç»“æœ
            match_result = _parse_match_result(result, data_item)
            
            section_index = match_result.get('section_index', -1)
            confidence = match_result.get('confidence_score', 0.0)
            
            if section_index >= 0:
                print(f"   âœ… [{item_index}/{total_items}] ID:{data_id} â†’ ç« èŠ‚[{section_index}] (ç½®ä¿¡åº¦:{confidence:.2f})")
            else:
                print(f"   âšª [{item_index}/{total_items}] ID:{data_id} â†’ ä¸åŒ¹é…")
            
            return match_result
            
        except Exception as e:
            print(f"   âŒ [{item_index}/{total_items}] ID:{data_id} å¤„ç†å¤±è´¥: {e}")
            return _create_error_result(data_item, str(e))


def _create_error_result(data_item: Dict[str, Any], error_msg: str) -> Dict[str, Any]:
    """åˆ›å»ºé”™è¯¯ç»“æœæ ¼å¼"""
    summary = data_item.get('summary', '')
    if not summary:
        content = data_item.get('content', '')
        summary = content[:100] + "..." if len(content) > 100 else content
    
    return {
        "data_id": data_item.get('id', ''),
        "section_index": -1,
        "section_title": "",
        "confidence_score": 0.0,
        "match_reason": f"å¤„ç†å¤±è´¥: {error_msg}",
        "content_summary": summary[:100],
        "error": True
    }


def _parse_match_result(result_text: str, data_item: Dict[str, Any]) -> Dict[str, Any]:
    """è§£æAIè¿”å›çš„åŒ¹é…ç»“æœ"""
    
    try:
        from financial_report.utils.extract_json_array import extract_json_array
        
        # ä½¿ç”¨extract_json_arrayå·¥å…·æå–JSON
        json_text = extract_json_array(result_text, mode='auto')
        
        if json_text:
            parsed_result = json.loads(json_text)
            
            # éªŒè¯å¿…è¦å­—æ®µå¹¶æ ‡å‡†åŒ–
            section_index = parsed_result.get('section_index', -1)
            section_title = parsed_result.get('section_title', '')
            confidence_score = parsed_result.get('confidence_score', 0.0)
            match_reason = parsed_result.get('match_reason', '')
            content_summary = parsed_result.get('content_summary', '')
            
            return {
                "data_id": data_item.get('id', ''),
                "section_index": section_index,
                "section_title": section_title,
                "confidence_score": confidence_score,
                "match_reason": match_reason,
                "content_summary": content_summary
            }
        
        # è§£æå¤±è´¥ï¼Œè¿”å›é»˜è®¤ç»“æœ
        return _create_error_result(data_item, "AIè¿”å›ç»“æœè§£æå¤±è´¥")
        
    except Exception as e:
        return _create_error_result(data_item, f"è§£æå¼‚å¸¸: {str(e)}")


def _build_final_outline(outline_data: Dict[str, Any], results: List[Dict[str, Any]]) -> Dict[str, Any]:
    """æ ¹æ®åŒ¹é…ç»“æœæ„å»ºæœ€ç»ˆå¤§çº²"""
    
    # åˆå§‹åŒ–å¤§çº² - å¤åˆ¶åŸå§‹æ•°æ®çš„æ‰€æœ‰å­—æ®µ
    final_outline = {key: value for key, value in outline_data.items() if key != "reportOutline"}
    final_outline["reportOutline"] = []
    
    # ä¸ºæ¯ä¸ªç« èŠ‚åˆå§‹åŒ–åˆ†é…åˆ—è¡¨
    sections = outline_data.get("reportOutline", [])
    for section in sections:
        final_section = {
            "title": section.get("title", ""),
            "points": section.get("points", []),
            "allocated_data_ids": [],
            "data_descriptions": {}
        }
        final_outline["reportOutline"].append(final_section)
    
    # æ·»åŠ ä¸åŒ¹é…ç« èŠ‚
    unmatched_section = {
        "title": "ä¸åŒ¹é…çš„æ•°æ®",
        "points": ["æœªèƒ½åŒ¹é…åˆ°åˆé€‚ç« èŠ‚çš„æ•°æ®é¡¹"],
        "allocated_data_ids": [],
        "data_descriptions": {}
    }
    final_outline["reportOutline"].append(unmatched_section)
    
    # æ ¹æ®åŒ¹é…ç»“æœåˆ†é…æ•°æ®
    for result in results:
        if isinstance(result, Exception) or result.get('error', False):
            continue
            
        data_id = result.get('data_id', '')
        section_index = result.get('section_index', -1)
        content_summary = result.get('content_summary', '')
        confidence_score = result.get('confidence_score', 0.0)
        match_reason = result.get('match_reason', '')
        
        if section_index >= 0 and section_index < len(sections):
            # åŒ¹é…åˆ°å…·ä½“ç« èŠ‚
            final_outline["reportOutline"][section_index]["allocated_data_ids"].append(data_id)
            description = f"{content_summary} (ç½®ä¿¡åº¦: {confidence_score:.2f}, ç†ç”±: {match_reason})"
            final_outline["reportOutline"][section_index]["data_descriptions"][data_id] = description
        else:
            # ä¸åŒ¹é…ï¼Œæ”¾åˆ°æœ€åä¸€ä¸ªç« èŠ‚
            final_outline["reportOutline"][-1]["allocated_data_ids"].append(data_id)
            description = f"{content_summary} (ä¸åŒ¹é…åŸå› : {match_reason})"
            final_outline["reportOutline"][-1]["data_descriptions"][data_id] = description
    
    return final_outline


def _generate_matching_stats(results: List[Dict[str, Any]], total_items: int) -> Dict[str, Any]:
    """ç”ŸæˆåŒ¹é…ç»Ÿè®¡ä¿¡æ¯"""
    
    matched_count = 0
    unmatched_count = 0
    failed_count = 0
    confidence_scores = []
    
    for result in results:
        if isinstance(result, Exception):
            failed_count += 1
        elif result.get('error', False):
            failed_count += 1
        else:
            section_index = result.get('section_index', -1)
            confidence_score = result.get('confidence_score', 0.0)
            
            if section_index >= 0:
                matched_count += 1
                confidence_scores.append(confidence_score)
            else:
                unmatched_count += 1
    
    match_rate = (matched_count / total_items * 100) if total_items > 0 else 0
    avg_confidence = sum(confidence_scores) / len(confidence_scores) if confidence_scores else 0.0
    
    return {
        "total_items": total_items,
        "matched_count": matched_count,
        "unmatched_count": unmatched_count,
        "failed_count": failed_count,
        "match_rate": match_rate,
        "avg_confidence": avg_confidence,
        "confidence_distribution": {
            "high": len([c for c in confidence_scores if c >= 0.8]),
            "medium": len([c for c in confidence_scores if 0.5 <= c < 0.8]),
            "low": len([c for c in confidence_scores if c < 0.5])
        }
    }


def allocate_data_to_outline_sync(
    outline_data: Dict[str, Any],
    flattened_data: List[Dict[str, Any]],
    api_key: str,
    base_url: str,
    model: str,
    max_output_tokens: int = 8 * 1024,
    max_concurrent: int = 10,
) -> Dict[str, Any]:
    """
    åŒæ­¥ç‰ˆæœ¬çš„åå‘åŒ¹é…å‡½æ•°
    
    Args:
        outline_data: å¤§çº²æ•°æ®å­—å…¸
        flattened_data: å±•å¹³åçš„æ•°æ®åˆ—è¡¨
        api_key: APIå¯†é’¥
        base_url: APIåŸºç¡€URL
        model: ä½¿ç”¨çš„æ¨¡å‹åç§°
        max_output_tokens: æœ€å¤§è¾“å‡ºtokens
        max_concurrent: æœ€å¤§å¹¶å‘æ•°
        **kwargs: ä¸ºäº†å…¼å®¹æ€§ä¿ç•™çš„å…¶ä»–å‚æ•°ï¼ˆå°†è¢«å¿½ç•¥ï¼‰
    
    Returns:
        åŒ…å«åˆ†é…ç»“æœçš„å­—å…¸
    """
    
    return asyncio.run(allocate_data_to_outline_reverse(
        outline_data=outline_data,
        flattened_data=flattened_data,
        api_key=api_key,
        base_url=base_url,
        model=model,
        max_output_tokens=max_output_tokens,
        max_concurrent=max_concurrent
    ))


def analyze_outline_coverage(allocation_result: Dict[str, Any]) -> Dict[str, Any]:
    """
    åˆ†æå¤§çº²æ•°æ®è¦†ç›–æƒ…å†µ
    
    Args:
        allocation_result: æ•°æ®åˆ†é…ç»“æœ
    
    Returns:
        åŒ…å«è¦†ç›–åˆ†æçš„å­—å…¸
    """
    outline_sections = allocation_result.get("outline_with_allocations", {}).get("reportOutline", [])
    
    filled_sections = []
    empty_sections = []
    
    for section in outline_sections:
        allocated_data_ids = section.get("allocated_data_ids", [])
        if allocated_data_ids:
            filled_sections.append(section)
        else:
            empty_sections.append(section)
    
    total_sections = len(outline_sections)
    filled_count = len(filled_sections)
    empty_count = len(empty_sections)
    coverage_rate = (filled_count / total_sections * 100) if total_sections > 0 else 0
    
    return {
        "total_sections": total_sections,
        "filled_sections": filled_sections,
        "empty_sections": empty_sections,
        "filled_count": filled_count,
        "empty_count": empty_count,
        "coverage_rate": coverage_rate
    }


if __name__ == "__main__":
    # æµ‹è¯•ç”¨ä¾‹
    import os
    from dotenv import load_dotenv
    
    load_dotenv()
    
    # ä½¿ç”¨æ™ºè°±AIè¿›è¡Œæµ‹è¯•
    api_key = os.getenv("ZHIPU_API_KEY")
    base_url = os.getenv("ZHIPU_BASE_URL")
    model = os.getenv("ZHIPU_FREE_TEXT_MODEL")
    
    # è¯»å–æµ‹è¯•æ•°æ®
    with open("test_company_datas/company_outline.json", "r", encoding="utf-8") as f:
        outline_data = json.load(f)
    
    with open("test_company_datas/flattened_tonghuashun_data.json", "r", encoding="utf-8") as f:
        flattened_data = json.load(f)
    
    # æµ‹è¯•å°‘é‡æ•°æ®
    test_data = flattened_data[:5]  # åªæµ‹è¯•å‰5ä¸ª
    
    result = allocate_data_to_outline_sync(
        outline_data=outline_data,
        flattened_data=test_data,
        api_key=api_key,
        base_url=base_url,
        model=model,
        max_concurrent=3
    )
    
    # ä¿å­˜ç»“æœ
    with open("test_company_datas/outline_data_allocation.json", "w", encoding="utf-8") as f:
        json.dump(result, f, ensure_ascii=False, indent=2)
    
    print("âœ… æµ‹è¯•å®Œæˆï¼")
