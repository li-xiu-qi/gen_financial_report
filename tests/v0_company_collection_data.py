import json
import requests
import os
import warnings

from dotenv import load_dotenv
from financial_report.llm_calls import generate_company_outline, company_outline_search_queries
from financial_report.search_tools.search_tools import (
    bing_search_with_cache,
    get_tonghuashun_data,
)
from financial_report.llm_calls.content_assessor import assess_content_quality_hybrid
from data_process.find_competitors import find_competitors
from data_process.content_summarizer import generate_summaries_for_collected_data
from data_process.outline_data_allocator import allocate_data_to_outline_sync
from data_process.company_visual_data_enhancer import CompanyVisualDataEnhancer
from data_process.company_visualization_data_processor import CompanyVisualizationDataProcessor

# æˆ‘ä»¬çš„å¤§æ¨¡å‹ç”Ÿæˆçš„å†…å®¹ç»Ÿä¸€ä¿å­˜åˆ°test_datasç›®å½•ä¸‹ï¼Œæˆ‘ä»¬å…ˆåˆ›å»ºè¿™ä¸ª
if not os.path.exists("test_company_datas"):
    os.mkdir("test_company_datas")

# ====== å¯¼å…¥åˆ° ReflectRAG ======
# å¯¼å…¥ç¯å¢ƒå˜é‡
load_dotenv()

# è·å–æœ¬åœ°embeddingæ¨¡å‹é…ç½®
local_api_key = os.getenv("LOCAL_API_KEY")
local_base_url = os.getenv("LOCAL_BASE_URL")
local_embedding_model = os.getenv("LOCAL_EMBEDDING_MODEL")

api_key = os.getenv("GUIJI_API_KEY")
base_url = os.getenv("GUIJI_BASE_URL")
model = os.getenv("GUIJI_FREE_TEXT_MODEL")
costly_model = os.getenv("GUIJI_TEXT_MODEL_DEEPSEEK_PRO")

max_output_tokens = int(8 * 1024)
# å¯¼å…¥search_urlå’Œpdf_base_url
search_url = os.getenv("SEARCH_URL")
pdf_base_url = os.getenv("PDF_BASE_URL")

zhipu_api_key = os.getenv("GUIJI_API_KEY")
zhipu_base_url = os.getenv("GUIJI_BASE_URL")
zhipu_model = os.getenv("GUIJI_TEXT_MODEL_DEEPSEEK_PRO")
zhipu_max_chat_tokens = int(128 * 1024 * 0.8)  # 128K * 0.8

# ====== ç»Ÿä¸€å¹¶å‘é…ç½® ======
MAX_CONCURRENT = 190  # ç»Ÿä¸€çš„æœ€å¤§å¹¶å‘æ•°

# å®šä¹‰ç›®æ ‡å…¬å¸ä¿¡æ¯
company_name = "4Paradigm"
company_code = "06682.HK"
target_company = company_name


# æ–‡ä»¶è·¯å¾„å®šä¹‰ï¼ˆå…¨éƒ¨jsonå’Œåˆ†æç»“æœç»Ÿä¸€åˆ° test_company_datasï¼Œå›¾ç‰‡ä¹Ÿç»Ÿä¸€åˆ° test_company_datas/imagesï¼‰
competitors_file = os.path.join("test_company_datas", "competitors.json")
company_outline_file = os.path.join("test_company_datas", "company_outline.json")
competitors_tonghuashun_data_file = os.path.join("test_company_datas", "competitors_tonghuashun_data.json")
flattened_tonghuashun_file = os.path.join("test_company_datas", "flattened_tonghuashun_data.json")
allocation_result_file = os.path.join("test_company_datas", "outline_data_allocation.json")
search_results_file = os.path.join("test_company_datas", "search_results_data.json")
enhanced_allocation_file = os.path.join("test_company_datas", "enhanced_allocation_result.json")
visual_enhancement_file = os.path.join("test_company_datas", "visual_enhancement_results.json")

# å›¾ç‰‡è¾“å‡ºç›®å½•
image_output_dir = os.path.join("test_company_datas", "images")
if not os.path.exists(image_output_dir):
    os.makedirs(image_output_dir, exist_ok=True)

# ====== è·¯å¾„é…ç½® ======
# å¯è§†åŒ–è¾“å‡ºè·¯å¾„é…ç½®
VISUALIZATION_HTML_OUTPUT_DIR = os.path.dirname(os.path.abspath(__file__))  # é¡¹ç›®æ ¹ç›®å½•ï¼Œä¸jsåŒçº§
VISUALIZATION_ASSETS_OUTPUT_DIR = image_output_dir  # PNGå’ŒJSONèµ„äº§è¾“å‡ºç›®å½•

print(f"ğŸ“ å¯è§†åŒ–è·¯å¾„é…ç½®:")
print(f"   HTMLè¾“å‡ºç›®å½•: {VISUALIZATION_HTML_OUTPUT_DIR}")
print(f"   èµ„äº§è¾“å‡ºç›®å½•: {VISUALIZATION_ASSETS_OUTPUT_DIR}")

print("=" * 60)
print("ğŸš€ å¯åŠ¨å…¬å¸ç ”ç©¶æŠ¥å‘Šæ•°æ®æ”¶é›†å’Œåˆ†é…æµç¨‹")
print("=" * 60)

# æ­¥éª¤1: è·å–ç«äº‰å¯¹æ‰‹
print("\n" + "="*50)
print("æ­¥éª¤ 1ï¼šè·å–ç«äº‰å¯¹æ‰‹")
print("="*50)

try:
    competitors_result = find_competitors(
        name=target_company,
        api_key=zhipu_api_key,
        base_url=zhipu_base_url,
        chat_model=zhipu_model,
        search_api_url=search_url
    )
    
    with open(competitors_file, "w", encoding="utf-8") as f:
        json.dump(competitors_result, f, ensure_ascii=False, indent=2)
    print(f"âœ… ç«äº‰å¯¹æ‰‹åˆ†æå®Œæˆ")
    print(f"ğŸ“ æ–‡ä»¶å·²ä¿å­˜åˆ°: {competitors_file}")
    
    if "competitors" in competitors_result:
        competitors_list = competitors_result["competitors"]
        print(f"ğŸ¢ æ‰¾åˆ°ç«äº‰å¯¹æ‰‹: {len(competitors_list)} å®¶")
        for i, comp in enumerate(competitors_list[:5], 1):
            print(f"   {i}. {comp.get('name', 'N/A')} - {comp.get('description', 'N/A')[:50]}...")
        if len(competitors_list) > 5:
            print(f"   ... è¿˜æœ‰ {len(competitors_list) - 5} å®¶ç«äº‰å¯¹æ‰‹")
    
except Exception as e:
    print(f"âŒ ç«äº‰å¯¹æ‰‹è·å–å¤±è´¥: {e}")
    competitors_result = {"competitors": []}

# æ­¥éª¤2: ç”Ÿæˆå…¬å¸å¤§çº²
print("\n" + "="*50)
print("æ­¥éª¤ 2ï¼šç”Ÿæˆå…¬å¸å¤§çº²")
print("="*50)

try:
    company_outline_result = generate_company_outline(
        company=target_company,
        company_code=company_code,
        api_key=zhipu_api_key,
        base_url=zhipu_base_url,
        model=zhipu_model,
        max_tokens=max_output_tokens
    )
    
    with open(company_outline_file, "w", encoding="utf-8") as f:
        json.dump(company_outline_result, f, ensure_ascii=False, indent=2)
    print(f"âœ… å…¬å¸å¤§çº²ç”Ÿæˆå®Œæˆ")
    print(f"ğŸ“ æ–‡ä»¶å·²ä¿å­˜åˆ°: {company_outline_file}")
    
    if "outline" in company_outline_result:
        outline_sections = company_outline_result["outline"]
        print(f"ğŸ“‹ å¤§çº²ç« èŠ‚: {len(outline_sections)} ä¸ª")
        for i, section in enumerate(outline_sections[:5], 1):
            print(f"   {i}. {section.get('title', 'N/A')}")
        if len(outline_sections) > 5:
            print(f"   ... è¿˜æœ‰ {len(outline_sections) - 5} ä¸ªç« èŠ‚")
    
except Exception as e:
    print(f"âŒ å…¬å¸å¤§çº²ç”Ÿæˆå¤±è´¥: {e}")
    company_outline_result = {"outline": []}

# æ­¥éª¤3: è·å–åŒèŠ±é¡ºæ•°æ®
print("\n" + "="*50)
print("æ­¥éª¤ 3ï¼šè·å–åŒèŠ±é¡ºæ•°æ®")
print("="*50)

try:
    # è¯»å–ç«äº‰å¯¹æ‰‹æ•°æ® 
    if os.path.exists(competitors_file):
        with open(competitors_file, "r", encoding="utf-8") as f:
            competitors_data = json.load(f)
            
        # å¤„ç†ä¸åŒçš„æ•°æ®æ ¼å¼
        if isinstance(competitors_data, dict) and "competitors" in competitors_data:
            competitors_list = competitors_data["competitors"]
        elif isinstance(competitors_data, list):
            competitors_list = competitors_data
        else:
            competitors_list = []
    else:
        competitors_list = []
    
    # æ„å»ºå…¬å¸åˆ—è¡¨
    all_companies = [{"name": target_company, "code": company_code}]
    
    for comp in competitors_list:
        if isinstance(comp, dict):
            comp_name = comp.get("company_name") or comp.get("name")
            comp_code = comp.get("tonghuashun_total_code") or comp.get("stock_code") or comp.get("code")
            
            if comp_name and comp_code:
                all_companies.append({
                    "name": comp_name,
                    "code": comp_code
                })
    
    print(f"ğŸ“Š å¼€å§‹è·å– {len(all_companies)} å®¶å…¬å¸çš„åŒèŠ±é¡ºæ•°æ®...")
    
    # ä¸ºæ¯ä¸ªå…¬å¸è·å–åŒèŠ±é¡ºæ•°æ®
    competitors_tonghuashun_data = {}
    for company in all_companies:
        try:
            company_data = get_tonghuashun_data(
                tonghuashun_total_code=company["code"],
                search_api_url=search_url
            )
            competitors_tonghuashun_data[company["name"]] = company_data
            print(f"âœ… è·å– {company['name']} æ•°æ®æˆåŠŸ")
        except Exception as e:
            print(f"âš ï¸  è·å– {company['name']} æ•°æ®å¤±è´¥: {e}")
            competitors_tonghuashun_data[company["name"]] = {"navs": [], "news": []}
    
    with open(competitors_tonghuashun_data_file, "w", encoding="utf-8") as f:
        json.dump(competitors_tonghuashun_data, f, ensure_ascii=False, indent=2)
    print(f"âœ… åŒèŠ±é¡ºæ•°æ®è·å–å®Œæˆ")
    print(f"ğŸ“ æ–‡ä»¶å·²ä¿å­˜åˆ°: {competitors_tonghuashun_data_file}")
    
    # ç»Ÿè®¡æ•°æ®ç‚¹
    total_navs = sum(len(data.get("navs", [])) for data in competitors_tonghuashun_data.values())
    total_news = sum(len(data.get("news", [])) for data in competitors_tonghuashun_data.values())
    print(f"ğŸ“ˆ è·å–æ•°æ®ç‚¹: å¯¼èˆª {total_navs} ä¸ªï¼Œæ–°é—» {total_news} ä¸ª")
    
except Exception as e:
    print(f"âŒ åŒèŠ±é¡ºæ•°æ®è·å–å¤±è´¥: {e}")
    competitors_tonghuashun_data = {}

# æ­¥éª¤4: å±•å¹³åŒèŠ±é¡ºæ•°æ®
print("\n" + "="*50)
print("æ­¥éª¤ 4ï¼šå±•å¹³åŒèŠ±é¡ºæ•°æ®")
print("="*50)

def flatten_tonghuashun_data(tonghuashun_data_dict: dict) -> list:
    """
    å°†åŒèŠ±é¡ºæ•°æ®å±•å¹³ä¸ºç»Ÿä¸€æ ¼å¼çš„æ•°æ®åˆ—è¡¨
    
    Args:
        tonghuashun_data_dict: å…¬å¸ååˆ°åŒèŠ±é¡ºæ•°æ®çš„æ˜ å°„
        
    Returns:
        å±•å¹³åçš„æ•°æ®åˆ—è¡¨
    """
    flattened_data = []
    current_id = 1
    
    for company_name, company_data in tonghuashun_data_dict.items():
        # å¤„ç†å¯¼èˆªæ•°æ® (navs)
        navs = company_data.get("navs", [])
        for nav_item in navs:
            flattened_record = {
                "id": str(current_id),
                "company_name": company_name,
                "company_code": "",
                "market": "",
                "tonghuashun_total_code": "",
                "url": nav_item.get("url", ""),
                "title": nav_item.get("title", ""),
                "data_source_type": nav_item.get("data_source_type", "html"),
                "content": nav_item.get("md", ""),
                "search_query": "",
                "data_source": "tonghuashun_nav"
            }
            flattened_data.append(flattened_record)
            current_id += 1
        
        # å¤„ç†æ–°é—»æ•°æ® (news)
        news = company_data.get("news", [])
        for news_item in news:
            flattened_record = {
                "id": str(current_id),
                "company_name": company_name,
                "company_code": "",
                "market": "",
                "tonghuashun_total_code": "",
                "url": news_item.get("url", ""),
                "title": news_item.get("title", ""),
                "data_source_type": news_item.get("data_source_type", "html"),
                "content": news_item.get("md", ""),
                "search_query": "",
                "data_source": "tonghuashun_news"
            }
            flattened_data.append(flattened_record)
            current_id += 1
    
    return flattened_data

try:
    flattened_data = flatten_tonghuashun_data(competitors_tonghuashun_data)
    
    with open(flattened_tonghuashun_file, "w", encoding="utf-8") as f:
        json.dump(flattened_data, f, ensure_ascii=False, indent=2)
    print(f"âœ… æ•°æ®å±•å¹³å®Œæˆ")
    print(f"ğŸ“ æ–‡ä»¶å·²ä¿å­˜åˆ°: {flattened_tonghuashun_file}")
    print(f"ğŸ“Š å±•å¹³åæ•°æ®é¡¹: {len(flattened_data)} æ¡")
    
except Exception as e:
    print(f"âŒ æ•°æ®å±•å¹³å¤±è´¥: {e}")
    flattened_data = []

# æ­¥éª¤5: æ•°æ®åˆ†é…åˆ°å¤§çº²
print("\n" + "="*50)
print("æ­¥éª¤ 5ï¼šæ•°æ®åˆ†é…åˆ°å¤§çº²")
print("="*50)

try:
    from data_process.outline_data_allocator import allocate_data_to_outline_sync
    
    allocation_result = allocate_data_to_outline_sync(
        outline_data=company_outline_result,
        flattened_data=flattened_data,
        api_key=zhipu_api_key,
        base_url=zhipu_base_url,
        model=zhipu_model,
        max_tokens_per_batch=zhipu_max_chat_tokens,
        max_concurrent=MAX_CONCURRENT
    )
    
    with open(allocation_result_file, "w", encoding="utf-8") as f:
        json.dump(allocation_result, f, ensure_ascii=False, indent=2)
    print(f"âœ… æ•°æ®åˆ†é…å®Œæˆ")
    print(f"ğŸ“ æ–‡ä»¶å·²ä¿å­˜åˆ°: {allocation_result_file}")
    
    stats = allocation_result.get("allocation_stats", {})
    print(f"ğŸ“Š åˆ†é…ç»Ÿè®¡:")
    print(f"   - åŒ¹é…æˆåŠŸ: {stats.get('matched_count', 0)}")
    print(f"   - æ€»ç« èŠ‚æ•°: {stats.get('total_sections', 0)}")
    print(f"   - åŒ¹é…ç‡: {stats.get('match_rate', 0):.1f}%")
    
except Exception as e:
    print(f"âŒ æ•°æ®åˆ†é…å¤±è´¥: {e}")
    allocation_result = {"allocated_sections": [], "allocation_stats": {}}

# æ­¥éª¤6: æ£€æŸ¥æ•°æ®è¦†ç›–ç‡
print("\n" + "="*50)
print("æ­¥éª¤ 6ï¼šåˆ†ææ•°æ®è¦†ç›–æƒ…å†µ")
print("="*50)

try:
    # åˆ†æè¦†ç›–ç‡ - ä¿®å¤æ•°æ®ç»“æ„è®¿é—®
    outline_with_allocations = allocation_result.get("outline_with_allocations", {})
    report_outline = outline_with_allocations.get("reportOutline", [])
    
    empty_sections = []
    filled_sections = []
    
    for section in report_outline:
        allocated_data_ids = section.get("allocated_data_ids", [])
        if allocated_data_ids and len(allocated_data_ids) > 0:
            filled_sections.append(section)
        else:
            empty_sections.append(section)
    
    coverage_analysis = {
        "empty_sections": empty_sections,
        "filled_sections": filled_sections,
        "total_sections": len(report_outline),
        "coverage_rate": len(filled_sections) / len(report_outline) * 100 if report_outline else 0
    }
    
    coverage_file = "test_company_datas/outline_coverage_analysis.json"
    with open(coverage_file, "w", encoding="utf-8") as f:
        json.dump(coverage_analysis, f, ensure_ascii=False, indent=2)
    
    print(f"âœ… è¦†ç›–ç‡åˆ†æå®Œæˆ")
    print(f"ğŸ“ åˆ†æç»“æœå·²ä¿å­˜åˆ°: {coverage_file}")
    print(f"ğŸ“Š æ•°æ®è¦†ç›–æƒ…å†µ:")
    print(f"   - å·²å¡«å……ç« èŠ‚: {len(filled_sections)}")
    print(f"   - ç©ºç™½ç« èŠ‚: {len(empty_sections)}")
    if report_outline:
        print(f"   - æ€»ä½“è¦†ç›–ç‡: {coverage_analysis['coverage_rate']:.1f}%")
    
    if empty_sections:
        print(f"\nâš ï¸ éœ€è¦è¡¥å……æ•°æ®çš„ç« èŠ‚:")
        for i, section in enumerate(empty_sections[:5], 1):
            print(f"   {i}. {section.get('title', 'N/A')}")
        if len(empty_sections) > 5:
            print(f"   ... è¿˜æœ‰ {len(empty_sections) - 5} ä¸ªç©ºç™½ç« èŠ‚")
    
except Exception as e:
    print(f"âŒ è¦†ç›–ç‡åˆ†æå¤±è´¥: {e}")
    empty_sections = []
    import traceback
    traceback.print_exc()

# æ­¥éª¤7: æ™ºèƒ½æœç´¢å¢å¼º
print("\n" + "="*50)
print("æ­¥éª¤ 7ï¼šæ™ºèƒ½æœç´¢å¢å¼º")
print("="*50)

if empty_sections and len(empty_sections) > 0:
    print(f"\nğŸ” å¼€å§‹ä¸º {len(empty_sections)} ä¸ªæ— æ•°æ®ç« èŠ‚è¿›è¡Œæ™ºèƒ½æœç´¢...")
    
    try:
        from data_process.search_data_processor import SearchDataProcessor
        
        # åˆ›å»ºæœç´¢æ•°æ®å¤„ç†å™¨
        search_processor = SearchDataProcessor(
            api_key=zhipu_api_key,
            base_url=zhipu_base_url,
            model=zhipu_model,
            summary_api_key=zhipu_api_key,
            summary_base_url=zhipu_base_url,
            summary_model=zhipu_model
        )
        
        # æ‰§è¡Œæ™ºèƒ½æœç´¢
        search_results = search_processor.smart_search_for_empty_sections(
            empty_sections=empty_sections,
            company_name=target_company,
            existing_flattened_data=flattened_data,
            search_api_url=search_url,
            chat_max_token_length=zhipu_max_chat_tokens,
            max_searches_per_section=3,
            max_results_per_search=10,
            max_concurrent_summary=MAX_CONCURRENT
        )
        
        # ä¿å­˜æœç´¢ç»“æœ
        with open(search_results_file, "w", encoding="utf-8") as f:
            json.dump(search_results, f, ensure_ascii=False, indent=2)
        print(f"âœ… æ™ºèƒ½æœç´¢å®Œæˆ")
        print(f"ğŸ“ æœç´¢ç»“æœå·²ä¿å­˜åˆ°: {search_results_file}")
        
        # åˆå¹¶æœç´¢æ•°æ®
        new_search_data = search_results.get("new_search_data", [])
        if new_search_data:
            print(f"ğŸ”— åˆå¹¶æœç´¢æ•°æ®...")
            enhanced_flattened_data = search_processor.merge_search_data_with_existing(
                existing_flattened_data=flattened_data,
                new_search_data=new_search_data
            )
            
            # ä¿å­˜å¢å¼ºåçš„å±•å¹³æ•°æ®
            enhanced_flattened_file = "test_company_datas/enhanced_flattened_data.json"
            with open(enhanced_flattened_file, "w", encoding="utf-8") as f:
                json.dump(enhanced_flattened_data, f, ensure_ascii=False, indent=2)
            print(f"ğŸ“ å¢å¼ºåæ•°æ®å·²ä¿å­˜åˆ°: {enhanced_flattened_file}")
            
            # é‡æ–°åˆ†é…æ•°æ®
            print(f"ğŸ”„ é‡æ–°åˆ†é…å¢å¼ºåçš„æ•°æ®...")
            enhanced_allocation = allocate_data_to_outline_sync(
                outline_data=company_outline_result,
                flattened_data=enhanced_flattened_data,
                api_key=zhipu_api_key,
                base_url=zhipu_base_url,
                model=zhipu_model,
                max_tokens_per_batch=zhipu_max_chat_tokens,
                max_concurrent=MAX_CONCURRENT
            )
            
            with open(enhanced_allocation_file, "w", encoding="utf-8") as f:
                json.dump(enhanced_allocation, f, ensure_ascii=False, indent=2)
            print(f"âœ… å¢å¼ºåˆ†é…å®Œæˆ")
            print(f"ğŸ“ ç»“æœå·²ä¿å­˜åˆ°: {enhanced_allocation_file}")
            
            # æ›´æ–°ç»Ÿè®¡ä¿¡æ¯
            enhanced_stats = enhanced_allocation.get("allocation_stats", {})
            print(f"ğŸ“Š å¢å¼ºåˆ†é…ç»Ÿè®¡:")
            print(f"   - åŒ¹é…æˆåŠŸ: {enhanced_stats.get('matched_count', 0)}")
            print(f"   - æ€»ç« èŠ‚æ•°: {enhanced_stats.get('total_sections', 0)}")
            print(f"   - åŒ¹é…ç‡: {enhanced_stats.get('match_rate', 0):.1f}%")
            
    except Exception as e:
        print(f"âŒ æ™ºèƒ½æœç´¢å¤±è´¥: {e}")
        print("å°†ç»§ç»­åç»­æµç¨‹...")
        import traceback
        traceback.print_exc()
else:
    print(f"\nğŸ‰ æ‰€æœ‰ç« èŠ‚éƒ½æœ‰æ•°æ®åˆ†é…ï¼Œæ— éœ€é¢å¤–æœç´¢ï¼")

# æ­¥éª¤8: å¯è§†åŒ–æ•°æ®å¢å¼º
print("\n" + "="*50)
print("æ­¥éª¤ 8ï¼šå¯è§†åŒ–æ•°æ®å¢å¼º")
print("="*50)
print(f"ğŸ¢ åˆ†æç›®æ ‡å…¬å¸: {company_name}")

try:
    # ç¡®å®šè¦ä½¿ç”¨çš„æœ€ç»ˆæ•°æ®
    final_flattened_data = None
    if os.path.exists("test_company_datas/enhanced_flattened_data.json"):
        print(f"\nğŸ“Š ä½¿ç”¨å¢å¼ºåçš„å±•å¹³æ•°æ®è¿›è¡Œå¯è§†åŒ–åˆ†æ...")
        with open("test_company_datas/enhanced_flattened_data.json", "r", encoding="utf-8") as f:
            final_flattened_data = json.load(f)
    elif flattened_data:
        print(f"\nğŸ“Š ä½¿ç”¨åŸå§‹å±•å¹³æ•°æ®è¿›è¡Œå¯è§†åŒ–åˆ†æ...")
        final_flattened_data = flattened_data
    else:
        print(f"\nâš ï¸  æ²¡æœ‰å¯ç”¨çš„å±•å¹³æ•°æ®ï¼Œè·³è¿‡å¯è§†åŒ–å¢å¼ºæ­¥éª¤")

    if final_flattened_data:
        # ç¡®å®šè¦ä½¿ç”¨çš„åˆ†é…ç»“æœ
        final_allocation_result = allocation_result
        if os.path.exists(enhanced_allocation_file):
            print(f"ğŸ“‹ ä½¿ç”¨å¢å¼ºåçš„åˆ†é…ç»“æœ...")
            with open(enhanced_allocation_file, "r", encoding="utf-8") as f:
                final_allocation_result = json.load(f)
        else:
            print(f"ğŸ“‹ ä½¿ç”¨åŸå§‹åˆ†é…ç»“æœ...")

        # åˆ›å»ºå…¬å¸å¯è§†åŒ–æ•°æ®å¢å¼ºå™¨
        visual_enhancer = CompanyVisualDataEnhancer(
            api_key=zhipu_api_key,
            base_url=zhipu_base_url,
            model=zhipu_model,
            outline_data=company_outline_result  # ä¼ å…¥å¤§çº²æ•°æ®
        )

        # è¿è¡Œå®Œæ•´çš„å¯è§†åŒ–æ•°æ®å¢å¼ºæµç¨‹
        print(f"ğŸ¯ ç›®æ ‡å…¬å¸: {company_name}")
        visual_enhancement_results = visual_enhancer.run_full_enhancement_process(
            flattened_data=final_flattened_data,
            target_name=company_name,  # æ˜ç¡®ä¼ é€’ç›®æ ‡å…¬å¸åç§°
            max_concurrent=MAX_CONCURRENT
        )

        # ä¿å­˜å¯è§†åŒ–å¢å¼ºç»“æœ
        with open(visual_enhancement_file, "w", encoding="utf-8") as f:
            json.dump(visual_enhancement_results, f, ensure_ascii=False, indent=2)

        print(f"âœ… å¯è§†åŒ–æ•°æ®å¢å¼ºå®Œæˆ")
        print(f"ğŸ“ ç»“æœå·²ä¿å­˜åˆ°: {visual_enhancement_file}")

        # æ˜¾ç¤ºå¯è§†åŒ–å»ºè®®ç»Ÿè®¡
        analysis_phase = visual_enhancement_results.get("analysis_phase", {})
        visualization_suggestions = analysis_phase.get("visualization_suggestions", [])
        print(f"ğŸ¨ ä¸º {company_name} ç”Ÿæˆå¯è§†åŒ–å»ºè®®: {len(visualization_suggestions)} æ¡")

        if visualization_suggestions:
            print(f"ğŸ“Š å¯è§†åŒ–ç±»å‹åˆ†å¸ƒ:")
            chart_types = {}
            for suggestion in visualization_suggestions:
                chart_type = suggestion.get("visualization_type", "æœªçŸ¥")
                chart_types[chart_type] = chart_types.get(chart_type, 0) + 1
            
            for chart_type, count in chart_types.items():
                print(f"   - {chart_type}: {count} ä¸ª")
            
            print(f"ğŸ“‹ ç« èŠ‚åˆ†å¸ƒ:")
            sections = {}
            for suggestion in visualization_suggestions:
                section = suggestion.get("section", "æœªåˆ†ç±»")
                sections[section] = sections.get(section, 0) + 1
            
            for section, count in sections.items():
                print(f"   - ç¬¬{section}ç« èŠ‚: {count} ä¸ª")
    else:
        print(f"âš ï¸  è·³è¿‡å¯è§†åŒ–æ•°æ®å¢å¼ºæ­¥éª¤")
        visual_enhancement_results = None

except Exception as e:
    print(f"âŒ å¯è§†åŒ–æ•°æ®å¢å¼ºå¤±è´¥: {e}")

# æ­¥éª¤8.5: å¯è§†åŒ–æ•°æ®å¤„ç† 
print("\n" + "="*50)
print("æ­¥éª¤ 8.5ï¼šå¯è§†åŒ–æ•°æ®å¤„ç†")  
print("="*50)
print(f"ğŸ¢ å¤„ç†ç›®æ ‡å…¬å¸: {company_name}")

try:
    # æ£€æŸ¥æ˜¯å¦æœ‰å¯è§†åŒ–å¢å¼ºç»“æœ
    if os.path.exists(visual_enhancement_file) and visual_enhancement_results:
        print(f"ğŸ“Š å¼€å§‹å¯è§†åŒ–æ•°æ®å¤„ç†...")
        
        # ç¡®å®šè¦ä½¿ç”¨çš„æ•°æ®
        final_data_for_viz = None
        if os.path.exists("test_datas/enhanced_flattened_data.json"):
            with open("test_company_datas/enhanced_flattened_data.json", "r", encoding="utf-8") as f:
                final_data_for_viz = json.load(f)
        elif flattened_data:
            final_data_for_viz = flattened_data
        
        if final_data_for_viz:
            # åˆ›å»ºå…¬å¸å¯è§†åŒ–æ•°æ®å¤„ç†å™¨ï¼ˆä½¿ç”¨é‡æ„åçš„ç±»ï¼‰
            viz_processor = CompanyVisualizationDataProcessor(
                api_key=zhipu_api_key,
                base_url=zhipu_base_url, 
                model=zhipu_model,
                visualization_output_dir=VISUALIZATION_HTML_OUTPUT_DIR,
                assets_output_dir=VISUALIZATION_ASSETS_OUTPUT_DIR
            )
            
            # å¤„ç†å¯è§†åŒ–æ•°æ®å¹¶ç”Ÿæˆå›¾è¡¨
            print(f"ğŸ¯ ç›®æ ‡å…¬å¸: {company_name}")
            viz_results = viz_processor.process_visualization_results(
                visual_enhancement_file=visual_enhancement_file,
                all_flattened_data=final_data_for_viz,
                target_name=company_name,  # æ˜ç¡®ä¼ é€’ç›®æ ‡å…¬å¸åç§°
                max_context_tokens=zhipu_max_chat_tokens,
                max_concurrent=MAX_CONCURRENT
            )
            
            # ä¿å­˜å¤„ç†ç»“æœ
            viz_results_file = "test_company_datas/visualization_data_results.json"
            with open(viz_results_file, "w", encoding="utf-8") as f:
                json.dump(viz_results, f, ensure_ascii=False, indent=2)
            
            print(f"âœ… å¯è§†åŒ–æ•°æ®å¤„ç†å®Œæˆ")
            print(f"ğŸ“ ç»“æœå·²ä¿å­˜åˆ°: {viz_results_file}")
            
            # ç»Ÿè®¡ç”Ÿæˆçš„å›¾è¡¨
            processing_summary = viz_results.get("processing_summary", {})
            successful_count = processing_summary.get("successful_count", 0)
            failed_count = processing_summary.get("failed_count", 0)
            
            print(f"ğŸ“ˆ ä¸º {company_name} ç”Ÿæˆå›¾è¡¨ç»Ÿè®¡:")
            print(f"   - æˆåŠŸç”Ÿæˆ: {successful_count} ä¸ª")
            print(f"   - ç”Ÿæˆå¤±è´¥: {failed_count} ä¸ª")
            
            # æ˜¾ç¤ºæˆåŠŸç”Ÿæˆçš„å›¾è¡¨è¯¦æƒ…
            processed_suggestions = viz_results.get("processed_suggestions", [])
            successful_charts = [s for s in processed_suggestions if s.get("success", False)]
            
            if successful_charts:
                print(f"ğŸ¨ ä¸º {company_name} æˆåŠŸç”Ÿæˆçš„å›¾è¡¨:")
                chart_types = {}
                sections = {}
                
                for chart in successful_charts:
                    chart_type = chart.get("visualization_type", "æœªçŸ¥")
                    section = chart.get("section", "æœªåˆ†ç±»")
                    chart_types[chart_type] = chart_types.get(chart_type, 0) + 1
                    sections[section] = sections.get(section, 0) + 1
                    
                    print(f"   - {chart.get('chart_title', 'Unknown')}")
                    print(f"     ç±»å‹: {chart_type}, ç« èŠ‚: ç¬¬{section}ç« èŠ‚")
                    print(f"     PNG: {'æœ‰' if chart.get('has_png', False) else 'æ— '}")
                
                print(f"\nğŸ“Š å›¾è¡¨ç±»å‹åˆ†å¸ƒ:")
                for chart_type, count in chart_types.items():
                    print(f"   - {chart_type}: {count} ä¸ª")
                
                print(f"\nğŸ“‹ ç« èŠ‚åˆ†å¸ƒ:")
                for section, count in sections.items():
                    print(f"   - ç¬¬{section}ç« èŠ‚: {count} ä¸ª")
                    
                # æ£€æŸ¥å›¾ç‰‡è¾“å‡ºç›®å½•
                if os.path.exists(image_output_dir):
                    image_files = [f for f in os.listdir(image_output_dir) if f.endswith('.png')]
                    json_files = [f for f in os.listdir(image_output_dir) if f.endswith('.json')]
                    print(f"\nğŸ“ å›¾è¡¨èµ„äº§:")
                    print(f"   - å›¾ç‰‡æ–‡ä»¶: {len(image_files)} ä¸ª")
                    print(f"   - é…ç½®æ–‡ä»¶: {len(json_files)} ä¸ª")
        else:
            print(f"âš ï¸  æ²¡æœ‰å¯ç”¨æ•°æ®è¿›è¡Œå¯è§†åŒ–å¤„ç†")
    else:
        print(f"âš ï¸  æ²¡æœ‰å¯è§†åŒ–å¢å¼ºç»“æœï¼Œè·³è¿‡æ•°æ®å¤„ç†æ­¥éª¤")
        
except Exception as e:
    print(f"âŒ å¯è§†åŒ–æ•°æ®å¤„ç†å¤±è´¥: {e}")

# æ­¥éª¤9: å›¾è¡¨åˆ†é…åŠŸèƒ½å·²é›†æˆåˆ°å¯è§†åŒ–æ•°æ®å¢å¼ºæ­¥éª¤ä¸­
print("\n" + "="*50)
print("æ­¥éª¤ 9ï¼šå›¾è¡¨åˆ†é…")
print("="*50)
print(f"âœ… å›¾è¡¨åˆ†é…åŠŸèƒ½å·²é›†æˆåˆ°å¯è§†åŒ–æ•°æ®å¢å¼ºæ­¥éª¤ä¸­")

print(f"\nğŸ‰ æ•°æ®æ”¶é›†å’Œåˆ†é…æµç¨‹å®Œæˆï¼")
print("ğŸ“ ç”Ÿæˆçš„æ–‡ä»¶:")
print(f"   - ç«äº‰å¯¹æ‰‹: {competitors_file}")
print(f"   - å…¬å¸å¤§çº²: {company_outline_file}")
print(f"   - åŒèŠ±é¡ºæ•°æ®: {competitors_tonghuashun_data_file}")
print(f"   - å±•å¹³æ•°æ®: {flattened_tonghuashun_file}")
print(f"   - åˆ†é…ç»“æœ: {allocation_result_file}")

# æ˜¾ç¤ºå¯é€‰çš„å¢å¼ºæ–‡ä»¶
if os.path.exists("test_company_datas/outline_coverage_analysis.json"):
    print(f"   - è¦†ç›–åˆ†æ: test_company_datas/outline_coverage_analysis.json")
if os.path.exists(search_results_file):
    print(f"   - æœç´¢ç»“æœ: {search_results_file}")
if os.path.exists(enhanced_allocation_file):
    print(f"   - å¢å¼ºåˆ†é…: {enhanced_allocation_file}")
if os.path.exists(visual_enhancement_file):
    print(f"   - å¯è§†åŒ–å¢å¼º: {visual_enhancement_file}")
if os.path.exists("test_company_datas/visualization_data_results.json"):
    print(f"   - å¯è§†åŒ–æ•°æ®æ”¶é›†: test_company_datas/visualization_data_results.json")

print(f"\nğŸ’¡ æ¨èä½¿ç”¨çš„æœ€ç»ˆæ•°æ®æ–‡ä»¶:")
if os.path.exists(enhanced_allocation_file):
    print(f"   ğŸ“Š ä½¿ç”¨å¢å¼ºåçš„åˆ†é…ç»“æœ: {enhanced_allocation_file}")
else:
    print(f"   ğŸ“Š ä½¿ç”¨åŸå§‹åˆ†é…ç»“æœ: {allocation_result_file}")

if os.path.exists(visual_enhancement_file):
    print(f"   ğŸ¨ å¯è§†åŒ–å¢å¼ºç»“æœ: {visual_enhancement_file}")

if os.path.exists("test_company_datas/visualization_data_results.json"):
    print(f"   ğŸ“Š å¯è§†åŒ–æ•°æ®æ”¶é›†: test_company_datas/visualization_data_results.json")

# æ˜¾ç¤ºå›¾è¡¨èµ„äº§ä¿¡æ¯
if os.path.exists(image_output_dir) and os.listdir(image_output_dir):
    png_files = [f for f in os.listdir(image_output_dir) if f.endswith('.png')]
    image_count = len(png_files)
    print(f"   ğŸ“ˆ å›¾è¡¨èµ„äº§: {image_output_dir}/ ({image_count} ä¸ªPNGå›¾è¡¨)")
else:
    print(f"   âš ï¸  æš‚æ— å›¾è¡¨èµ„äº§")
